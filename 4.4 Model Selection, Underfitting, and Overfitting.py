# 4.4. æ¨¡å‹é€‰æ‹©ã€æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆ
print("4.4. æ¨¡å‹é€‰æ‹©ã€æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆ")
print("4.4.4. å¤šé¡¹å¼å›å½’")
print('-' * 50)

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import math
import matplotlib.pyplot as plt
from matplotlib import rcParams
#å¾®è½¯é›…é»‘
rcParams['font.family'] = 'Microsoft YaHei'


# ä¸ºäº†å¯é‡å¤æ€§ï¼Œè®¾ç½®éšæœºç§å­
torch.manual_seed(0)
np.random.seed(0)

# å¤šé¡¹å¼çš„æœ€å¤§é˜¶æ•°
max_degree = 20

# è®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬æ•°é‡
n_train, n_test = 100, 100

# ç”¨äºç”Ÿæˆæ•°æ®çš„çœŸå®æƒé‡ï¼ˆåªæœ‰å‰4ä¸ªæ˜¯éé›¶çš„ï¼‰
true_w = np.zeros(max_degree)   # åˆå§‹åŒ–çœŸå®æƒé‡ï¼Œæ‰€æœ‰æƒé‡åˆå§‹åŒ–ä¸º0ï¼Œshapeä¸º (max_degree,)
true_w[0:4] = np.array([5, 1.2, -3.4, 5.6]) # è®¾ç½®å‰4ä¸ªæƒé‡ä¸º [5, 1.2, -3.4, 5.6]

# ç”Ÿæˆç‰¹å¾ x ~ N(0, 1)
features = np.random.normal(size=(n_train + n_test, 1)) # ç”Ÿæˆç‰¹å¾,np.random.normal ç”Ÿæˆæ­£æ€åˆ†å¸ƒçš„éšæœºæ•°ï¼Œå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1
#å¦‚æœæƒ³è¦ç”Ÿæˆå‡å€¼ä¸º1ï¼Œæ ‡å‡†å·®ä¸º2çš„æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼Œå¯ä»¥ä½¿ç”¨ np.random.normal(loc=1, scale=2, size=(n_train + n_test, 1)) locå…¨ç§°locationï¼Œscaleå…¨ç§°scale,locæ˜¯å‡å€¼ï¼Œscaleæ˜¯æ ‡å‡†å·®
np.random.shuffle(features) #æ‰“ä¹±æ•°æ®é¡ºåº

# ç”Ÿæˆå¤šé¡¹å¼ç‰¹å¾ï¼Œæœ€é«˜åˆ° max_degree æ¬¡æ–¹
poly_features = np.power(features, np.arange(max_degree))   #np.power(features, np.arange(max_degree)) è®¡ç®— features çš„å¹‚æ¬¡æ–¹ï¼Œnp.arange(max_degree) ç”Ÿæˆä» 0 åˆ° max_degree-1 çš„æ•°ç»„,ä¹Ÿå°±æ˜¯è®¡ç®— features çš„ 0 æ¬¡æ–¹åˆ° max_degree-1 æ¬¡æ–¹
#poly_featuresçš„shape: (n_train+n_test, max_degree)
# ä½¿ç”¨é˜¶ä¹˜è¿›è¡Œæ ‡å‡†åŒ–
for i in range(max_degree):    # range(n) ç”Ÿæˆä» 0 åˆ° n-1 çš„æ•°ç»„
    poly_features[:, i] /= math.gamma(i + 1)  # gamma(n) = (n-1)!
#ï¼»i:j, k:lï¼½è¡¨ç¤ºå–ç¬¬iåˆ°jè¡Œï¼Œç¬¬kåˆ°låˆ—çš„å…ƒç´ 
# ä½¿ç”¨ true_w ç”Ÿæˆæ ‡ç­¾ï¼Œå¹¶æ·»åŠ é«˜æ–¯å™ªå£°
#features np.powerå‰:    [[x1], [x2], [x3],...,[x(n_train + n_test)]]
#np.poweråï¼š[[x1^0,x1^1,x1^2,x1^3â€¦â€¦x^(max_degree-1)], [x2^0,x2^1,x2^2,x2^3â€¦â€¦x^(max_degree-1)], [x3^0,x3^1,x3^2,x3^3â€¦â€¦x^(max_degree-1)],...,[x(n_train + n_test)^0,x(n_train + n_test)^1,x(n_train + n_test)^2,x(n_train + n_test)^3â€¦â€¦x^(max_degree-1)]]
#math.gammaå:   [[x1^0/gamma(1),x1^1/gamma(2),x1^2/gamma(3),x1^3/gamma(4)â€¦â€¦x^(max_degree-1)/gamma(max_degree)], [x2^0/gamma(1),x2^1/gamma(2),x2^2/gamma(3),x2^3/gamma(4)â€¦â€¦x^(max_degree-1)/gamma(max_degree)], [x3^0/gamma(1),x3^1/gamma(2),x3^2/gamma(3),x3^3/gamma(4)â€¦â€¦x^(max_degree-1)/gamma(max_degree)],...,[x(n_train + n_test)^0/gamma(1),x(n_train + n_test)^1/gamma(2),x(n_train + n_test)^2/gamma(3),x(n_train + n_test)^3/gamma(4)â€¦â€¦x^(max_degree-1)/gamma(max_degree)]]

#shape  poly_features: (n_train + n_test, max_degree)   true_w: (max_degree,)   labels: (n_train + n_test,)
labels = np.dot(poly_features, true_w)  #è¿™é‡Œæ‰§è¡Œç‚¹ç§¯æ“ä½œåŸå› ï¼špoly_features çš„æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ¯ä¸€åˆ—æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œtrue_w æ˜¯ä¸€ä¸ªæƒé‡å‘é‡ï¼Œpoly_features å’Œ true_w çš„ç‚¹ç§¯å°±æ˜¯ poly_features çš„æ¯ä¸€è¡Œçš„çº¿æ€§ç»„åˆï¼Œä¹Ÿå°±æ˜¯ poly_features çš„æ¯ä¸€è¡Œçš„çº¿æ€§å›å½’
#np.dot(poly_features, true_w) è®¡ç®— poly_features å’Œ true_w çš„ç‚¹ç§¯ï¼Œä¹Ÿå°±æ˜¯è®¡ç®— poly_features çš„æ¯ä¸€è¡Œå’Œ true_w çš„ç‚¹ç§¯ï¼Œä¹Ÿå°±æ˜¯è®¡ç®— poly_features çš„æ¯ä¸€è¡Œçš„çº¿æ€§ç»„åˆï¼Œä¹Ÿå°±æ˜¯è®¡ç®— poly_features çš„æ¯ä¸€è¡Œçš„çº¿æ€§å›å½’
labels += np.random.normal(scale=0.1, size=labels.shape)    # åŠ ä¸Šé«˜æ–¯å™ªå£°ï¼Œscale=0.1 è¡¨ç¤ºæ ‡å‡†å·®ä¸º0.1ï¼Œsize=labels.shape è¡¨ç¤ºç”Ÿæˆçš„å™ªå£°çš„å½¢çŠ¶å’Œ labels çš„å½¢çŠ¶ç›¸åŒ

#å‡è®¾features = [[1], [2], [3]]   shape:(3, 1)
# np.arange(max_degree) = [0, 1, 2, 3, 4]   shape:(max_degree,)=(5,)
# åˆ™poly_features = np.dot(features, true_w) =
# å¯¹äºæ¯ä¸€ä¸ª features[i]ï¼Œæˆ‘ä»¬ä¼šè®¡ç®—å®ƒçš„ 0 æ¬¡å¹‚ã€1 æ¬¡å¹‚ã€2 æ¬¡å¹‚ã€3 æ¬¡å¹‚ã€4 æ¬¡å¹‚ï¼š
# å¯¹äº features[0] = 1ï¼š
# 1^0 = 1
# 1^1 = 1
# 1^2 = 1
# 1^3 = 1
# 1^4 = 1
# å¯¹äº features[1] = 2ï¼š
# 2^0 = 1
# 2^1 = 2
# 2^2 = 4
# 2^3 = 8
# 2^4 = 16
# å¯¹äº features[2] = 3ï¼š
# 3^0 = 1
# 3^1 = 3
# 3^2 = 9
# 3^3 = 27
# 3^4 = 81
# å› æ­¤ï¼Œpoly_features å°†ä¼šæ˜¯ï¼š
# poly_features = np.array([[1, 1, 1, 1, 1],
#                           [1, 2, 4, 8, 16],
#                           [1, 3, 9, 27, 81]])
#shape of poly_features: (3,5)
# A æ˜¯ (m, n)ï¼ŒB æ˜¯ (n, p) â†’ ç‚¹ç§¯ç»“æœæ˜¯ (m, p)ã€‚
# A æ˜¯ (m, n)ï¼ŒB æ˜¯ (n,) â†’ ç‚¹ç§¯ç»“æœæ˜¯ (m,)ã€‚
# A æ˜¯ (n,)ï¼ŒB æ˜¯ (n,) â†’ ç‚¹ç§¯ç»“æœæ˜¯æ ‡é‡ã€‚

# å°† numpy æ•°ç»„è½¬æ¢ä¸º torch å¼ é‡
features = torch.tensor(features, dtype=torch.float32)
poly_features = torch.tensor(poly_features, dtype=torch.float32)
labels = torch.tensor(labels, dtype=torch.float32)

# å‰ä¸¤ä¸ªæ ·æœ¬
print("å‰ä¸¤ä¸ªæ ·æœ¬:")
print("features:", features[:2])
print("poly_features:", poly_features[:2])
print("labels:", labels[:2])    #labels[:2]è¾“å‡ºshanpeä¸ºï¼š(2,),å› ä¸ºlabelsæ˜¯ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œå®ƒçš„shapeä¸º (n_train + n_test,)
print('-' * 50)
#å®é™…æ„ä¹‰ï¼š
#features: è¡¨ç¤ºä¸¤ä¸ªæ ·æœ¬çš„ç‰¹å¾ï¼Œæ¯ä¸ªæ ·æœ¬åªæœ‰ä¸€ä¸ªç‰¹å¾ï¼Œè¿™ä¸ªç‰¹å¾æ˜¯ä¸€ä¸ªéšæœºæ•°ï¼Œæœä»æ­£æ€åˆ†å¸ƒï¼Œå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1
#poly_features: è¡¨ç¤ºä¸¤ä¸ªæ ·æœ¬çš„å¤šé¡¹å¼ç‰¹å¾ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ max_degree ä¸ªç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾æ˜¯ features çš„å¹‚æ¬¡æ–¹ï¼Œä» 0 æ¬¡æ–¹åˆ° max_degree-1 æ¬¡æ–¹ï¼Œå¹¶ä¸”æ¯ä¸ªç‰¹å¾éƒ½é™¤ä»¥äº†ç›¸åº”çš„é˜¶ä¹˜
#labels: è¡¨ç¤ºä¸¤ä¸ªæ ·æœ¬çš„æ ‡ç­¾ï¼Œæ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾æ˜¯ poly_features çš„æ¯ä¸€è¡Œå’Œ true_w çš„ç‚¹ç§¯ï¼Œä¹Ÿå°±æ˜¯ poly_features çš„æ¯ä¸€è¡Œçš„çº¿æ€§ç»„åˆï¼Œä¹Ÿå°±æ˜¯ poly_features çš„æ¯ä¸€è¡Œçš„çº¿æ€§å›å½’ï¼Œå¹¶ä¸”æ¯ä¸ªæ ‡ç­¾éƒ½åŠ ä¸Šäº†é«˜æ–¯å™ªå£°ï¼Œæœä»æ­£æ€åˆ†å¸ƒï¼Œå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º0.1

# å®šä¹‰è¯„ä¼°æŸå¤±çš„å‡½æ•°
def evaluate_loss(net, data_iter, loss_fn):
    """åœ¨ç»™å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„æŸå¤±ã€‚

    å‚æ•°:
    net (torch.nn.Module): ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚
    data_iter (DataLoader): æ•°æ®åŠ è½½å™¨ã€‚
    loss_fn: æŸå¤±å‡½æ•°ã€‚

    è¿”å›:
    float: æ•°æ®é›†ä¸Šçš„å¹³å‡æŸå¤±ã€‚
    """
    total_loss = 0.0    # æŸå¤±å’Œ
    n_samples = 0     # æ ·æœ¬æ•°é‡
    with torch.no_grad():
        for X, y in data_iter:  #Xæ˜¯ç‰¹å¾ï¼Œyæ˜¯æ ‡ç­¾,æ ‡ç­¾çš„å½¢çŠ¶ä¸º (n,)ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸º (n, 1) çš„å½¢çŠ¶ï¼Œä»¥ä¾¿äºæŸå¤±å‡½æ•°è®¡ç®—
            # æ³¨æ„è¿™é‡Œçš„ y.view(-1, 1) æ˜¯ä¸ºäº†å°† y è½¬æ¢ä¸º (n, 1) çš„å½¢çŠ¶ï¼Œå› ä¸ºæŸå¤±å‡½æ•°è¦æ±‚è¾“å…¥çš„æ ‡ç­¾å½¢çŠ¶ä¸º (n, 1)
            y_pred = net(X) #shape: (n, 1)
            loss = loss_fn(y_pred, y.view(-1, 1))   #loss_fn(y_pred, y.view(-1, 1)) è®¡ç®—é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„æŸå¤±ï¼Œy.view(-1, 1) å°† y è½¬æ¢ä¸º (n, 1) çš„å½¢çŠ¶ï¼Œå› ä¸ºæŸå¤±å‡½æ•°è¦æ±‚è¾“å…¥çš„æ ‡ç­¾å½¢çŠ¶ä¸º (n, 1)
            total_loss += loss.item() * y.shape[0]    #total_loss += loss.item() * y.shape[0] è®¡ç®—æŸå¤±çš„æ€»å’Œï¼Œloss.item() æ˜¯å°†æŸå¤±è½¬æ¢ä¸ºæ ‡é‡ï¼Œy.shape[0] æ˜¯å½“å‰æ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡
            n_samples += y.shape[0]    #n_samples += y.shape[0] æ˜¯è®¡ç®—æ ·æœ¬æ•°é‡çš„æ€»å’Œï¼Œy.shape[0] æ˜¯å½“å‰æ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡
            #y.shape[0]è¡¨ç¤ºyçš„è¡Œæ•°ï¼Œä¹Ÿå°±æ˜¯å½“å‰æ‰¹æ¬¡çš„æ ·æœ¬æ•°é‡
    return total_loss / n_samples    #è¿”å›æ•°æ®é›†ä¸Šçš„å¹³å‡æŸå¤±


# è®­ç»ƒå‡½æ•°  åŠŸèƒ½åŒ…æ‹¬ï¼šè®­ç»ƒæ¨¡å‹ã€è¯„ä¼°æ¨¡å‹ã€ç»˜åˆ¶æŸå¤±æ›²çº¿ã€æ‰“å°å­¦ä¹ åˆ°çš„æƒé‡
def train(train_features, test_features, train_labels, test_labels, num_epochs=400):
    """è®­ç»ƒæ¨¡å‹å¹¶ç»˜åˆ¶è®­ç»ƒå’Œæµ‹è¯•æŸå¤±ã€‚

    å‚æ•°:
    train_features (Tensor): è®­ç»ƒç‰¹å¾ã€‚
    test_features (Tensor): æµ‹è¯•ç‰¹å¾ã€‚
    train_labels (Tensor): è®­ç»ƒæ ‡ç­¾ã€‚
    test_labels (Tensor): æµ‹è¯•æ ‡ç­¾ã€‚
    num_epochs (int): è®­ç»ƒçš„è½®æ•°ã€‚
    """

    #ä¸€èˆ¬è€Œè¨€ï¼Œè®­ç»ƒdataçš„shape[0]æ˜¯dataä¸­æ•°æ®çš„è¡Œæ•°ï¼Œä¹Ÿå°±æ˜¯æ ·æœ¬æ•°é‡ï¼Œshape[1]æ˜¯dataä¸­æ•°æ®çš„åˆ—æ•°ï¼Œä¹Ÿå°±æ˜¯ç‰¹å¾æ•°é‡,è¾“å…¥ç»´åº¦

    # å®šä¹‰æŸå¤±å‡½æ•°
    loss_fn = nn.MSELoss()    #nn.MSELoss() æ˜¯å‡æ–¹è¯¯å·®æŸå¤±å‡½æ•°ï¼Œå®ƒçš„è¾“å…¥æ˜¯é¢„æµ‹å€¼å’ŒçœŸå®å€¼ï¼Œè¾“å‡ºæ˜¯å®ƒä»¬ä¹‹é—´çš„è¯¯å·®

    # å®šä¹‰æ¨¡å‹
    net = nn.Linear(train_features.shape[1], 1, bias=False)    #nn.Linearæ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå®ƒçš„è¾“å…¥ç»´åº¦æ˜¯train_features.shape[1]ï¼Œè¾“å‡ºç»´åº¦æ˜¯1,bias=Falseè¡¨ç¤ºä¸ä½¿ç”¨åç½®

    # åˆå§‹åŒ–æƒé‡
    for param in net.parameters():    #net.parameters() æ˜¯netçš„æ‰€æœ‰å‚æ•°ï¼ŒåŒ…æ‹¬æƒé‡å’Œåç½®
        # åˆå§‹åŒ–æƒé‡ä¸ºæ­£æ€åˆ†å¸ƒï¼Œå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º0.01
        nn.init.normal_(param, mean=0, std=0.01)    #nn.init.normal_(param, mean=0, std=0.01) æ˜¯åˆå§‹åŒ–æƒé‡ä¸ºæ­£æ€åˆ†å¸ƒï¼Œå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º0.01
        #å› ä¸ºè¿™é‡Œnetçš„bias=Falseï¼Œæ‰€ä»¥è¿™é‡Œåªåˆå§‹åŒ–æƒé‡ï¼Œä¸åˆå§‹åŒ–åç½®
        #è‹¥bias=Trueï¼Œåˆ™è¿˜éœ€è¦åˆå§‹åŒ–åç½®ï¼Œåˆå§‹åŒ–åç½®ä¸ºå¸¸æ•°ï¼Œå€¼ä¸º0
        #è‹¥æƒ³è®©åç½®ç­‰äº1ï¼Œåˆ™å¯ä»¥å†™æˆ nn.init.constant_(param, 1)

    # å®šä¹‰ä¼˜åŒ–å™¨
    optimizer = optim.SGD(net.parameters(), lr=0.01)    #optim.SGDæ˜¯éšæœºæ¢¯åº¦ä¸‹é™ä¼˜åŒ–å™¨ï¼Œå®ƒçš„è¾“å…¥æ˜¯æ¨¡å‹çš„å‚æ•°å’Œå­¦ä¹ ç‡ï¼Œè¾“å‡ºæ˜¯ä¼˜åŒ–å™¨

    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    batch_size = min(10, train_labels.shape[0])    #batch_size = min(10, train_labels.shape[0]) æ˜¯ä¸ºäº†é˜²æ­¢ batch_size è¶…è¿‡æ•°æ®çš„æ ·æœ¬æ•°é‡ï¼Œè¿™é‡Œå– batch_size ä¸º 10 å’Œæ•°æ®çš„æ ·æœ¬æ•°é‡ä¸­è¾ƒå°çš„é‚£ä¸ª
    dataset = torch.utils.data.TensorDataset(train_features, train_labels)    #TensorDatasetæ˜¯ä¸€ä¸ªæ•°æ®é›†ï¼Œå®ƒå°†æ•°æ®å’Œæ ‡ç­¾æ‰“åŒ…æˆä¸€ä¸ªæ•°æ®é›†ï¼Œä»¥ä¾¿äºæ‰¹é‡å¤„ç†,è¿™æ˜¯ä¸ªè¿­ä»£å™¨
    train_iter = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)    #DataLoaderæ˜¯ä¸€ä¸ªæ•°æ®åŠ è½½å™¨ï¼Œå®ƒå°†æ•°æ®å’Œæ ‡ç­¾æ‰“åŒ…æˆä¸€ä¸ªæ•°æ®é›†ï¼Œä»¥ä¾¿äºæ‰¹é‡å¤„ç†

    test_dataset = torch.utils.data.TensorDataset(test_features, test_labels)
    test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # å­˜å‚¨æŸå¤±
    train_losses = []
    test_losses = []

    for epoch in range(num_epochs):   #for epoch in range(num_epochs) æ˜¯è®­ç»ƒçš„è½®æ¬¡ï¼Œnum_epochs æ˜¯è®­ç»ƒçš„è½®æ¬¡
        net.train()    #net.train() æ˜¯å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ï¼Œè¿™æ ·å¯ä»¥å¯ç”¨ä¸€äº›ç‰¹æ®Šçš„å±‚ï¼Œæ¯”å¦‚ Dropout å’Œ BatchNorm
        for X, y in train_iter:
            optimizer.zero_grad()    #optimizer.zero_grad() æ˜¯å°†ä¼˜åŒ–å™¨çš„æ¢¯åº¦æ¸…é›¶ï¼Œè¿™æ ·å¯ä»¥é˜²æ­¢æ¢¯åº¦ç´¯ç§¯
            y_pred = net(X)    #y_pred = net(X) æ˜¯å°†ç‰¹å¾ X è¾“å…¥åˆ°æ¨¡å‹ net ä¸­ï¼Œå¾—åˆ°é¢„æµ‹å€¼ y_pred
            loss = loss_fn(y_pred, y.view(-1, 1))    #loss = loss_fn(y_pred, y.view(-1, 1)) è®¡ç®—é¢„æµ‹å€¼å’ŒçœŸå®å€¼çš„æŸå¤±ï¼Œy.view(-1, 1) å°† y è½¬æ¢ä¸º (n, 1) çš„å½¢çŠ¶ï¼Œå› ä¸ºæŸå¤±å‡½æ•°è¦æ±‚è¾“å…¥çš„æ ‡ç­¾å½¢çŠ¶ä¸º (n, 1)
            loss.backward()    #loss.backward() æ˜¯è®¡ç®—æ¢¯åº¦ï¼Œè¿™æ ·å¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æƒé‡
            optimizer.step()    #optimizer.step() æ˜¯æ›´æ–°æƒé‡ï¼Œè¿™æ ·å¯ä»¥é€šè¿‡æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°æƒé‡

        # åœ¨å½“å‰è½®æ¬¡è¯„ä¼°æŸå¤±
        net.eval()    #net.eval() æ˜¯å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼Œè¿™æ ·å¯ä»¥ç¦ç”¨ä¸€äº›ç‰¹æ®Šçš„å±‚ï¼Œæ¯”å¦‚ Dropout å’Œ BatchNorm
        train_loss = evaluate_loss(net, train_iter, loss_fn)    #train_loss = evaluate_loss(net, train_iter, loss_fn) è®¡ç®—è®­ç»ƒé›†ä¸Šçš„å¹³å‡æŸå¤±
        test_loss = evaluate_loss(net, test_iter, loss_fn)    #test_loss = evaluate_loss(net, test_iter, loss_fn) è®¡ç®—æµ‹è¯•é›†ä¸Šçš„å¹³å‡æŸå¤±
        train_losses.append(train_loss)    #train_losses.append(train_loss) æ˜¯å°†è®­ç»ƒé›†ä¸Šçš„å¹³å‡æŸå¤±æ·»åŠ åˆ°åˆ—è¡¨ train_losses ä¸­
        test_losses.append(test_loss)    #test_losses.append(test_loss) æ˜¯å°†æµ‹è¯•é›†ä¸Šçš„å¹³å‡æŸå¤±æ·»åŠ åˆ°åˆ—è¡¨ test_losses ä¸­

        # å¯é€‰åœ°æ‰“å°è¿›åº¦
        if epoch == 0 or (epoch + 1) % 20 == 0:    #epoch == 0 or (epoch + 1) % 20 == 0 æ˜¯ä¸ºäº†æ¯éš” 20 è½®æ¬¡æ‰“å°ä¸€æ¬¡è¿›åº¦
            print(f'è½®æ¬¡ {epoch + 1}, è®­ç»ƒæŸå¤±: {train_loss:.6f}, æµ‹è¯•æŸå¤±: {test_loss:.6f}')

    # æ‰“å°å­¦ä¹ åˆ°çš„æƒé‡
    w = net.weight.data.numpy()    #net.weight.data.numpy() æ˜¯å°†æ¨¡å‹çš„æƒé‡è½¬æ¢ä¸º numpy æ•°ç»„
    print('å­¦ä¹ åˆ°çš„æƒé‡:', w)

    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure()
    plt.semilogy(range(1, num_epochs + 1), train_losses, label='è®­ç»ƒæŸå¤±')  #semilogy æ˜¯ y è½´å¯¹æ•°åˆ»åº¦ï¼Œx è½´æ˜¯è½®æ¬¡ï¼Œy è½´æ˜¯æŸå¤±
    plt.semilogy(range(1, num_epochs + 1), test_losses, label='æµ‹è¯•æŸå¤±')
    plt.xlabel('è½®æ¬¡')
    plt.ylabel('æŸå¤±')
    plt.legend()    #legend æ˜¯å›¾ä¾‹ï¼Œlabel æ˜¯å›¾ä¾‹çš„æ ‡ç­¾
    plt.show()


# åˆ’åˆ†æ•°æ®é›†
train_poly_features = poly_features[:n_train]   #å–å‰n_trainä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒé›†
test_poly_features = poly_features[n_test:]    #å–ån_testä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†

train_labels = labels[:n_train]   #å–å‰n_trainä¸ªæ ·æœ¬ä½œä¸ºè®­ç»ƒé›†çš„æ ‡ç­¾
test_labels = labels[n_test:]     #å–ån_testä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•é›†çš„æ ‡ç­¾

# æ‹Ÿåˆä¸‰é˜¶å¤šé¡¹å¼ï¼ˆæ­£å¸¸æƒ…å†µï¼‰
print("æ‹Ÿåˆä¸‰é˜¶å¤šé¡¹å¼ï¼ˆæ­£å¸¸æƒ…å†µï¼‰")
print('-' * 50)
train(train_poly_features[:, :4], test_poly_features[:, :4], train_labels, test_labels)
print('-' * 50)

# æ‹Ÿåˆçº¿æ€§æ¨¡å‹ï¼ˆæ¬ æ‹Ÿåˆï¼‰
print("æ‹Ÿåˆçº¿æ€§æ¨¡å‹ï¼ˆæ¬ æ‹Ÿåˆï¼‰")
print('-' * 50)
train(train_poly_features[:, :2], test_poly_features[:, :2], train_labels, test_labels)
print('-' * 50)

# æ‹Ÿåˆ20é˜¶å¤šé¡¹å¼ï¼ˆè¿‡æ‹Ÿåˆï¼‰
print("æ‹Ÿåˆ20é˜¶å¤šé¡¹å¼ï¼ˆè¿‡æ‹Ÿåˆï¼‰")
print('-' * 50)
train(train_poly_features, test_poly_features, train_labels, test_labels, num_epochs=1500)
print('-' * 50)
'''
è¿™ä¸‰ä¸ª train å‡½æ•°è°ƒç”¨ä¸­çš„åŒºåˆ«ä¸»è¦åœ¨äºè¾“å…¥çš„ç‰¹å¾æ•°æ®çš„ç»´åº¦ä¸åŒï¼Œè¿™ç›´æ¥å½±å“æ¨¡å‹çš„å¤æ‚åº¦ä»¥åŠæ‹Ÿåˆçš„æ•ˆæœã€‚è®©æˆ‘ä»¬é€æ­¥åˆ†ææ¯ä¸ªè®­ç»ƒè¿‡ç¨‹çš„å·®å¼‚å’ŒåŸå› ï¼š

æ‹Ÿåˆä¸‰é˜¶å¤šé¡¹å¼ï¼ˆæ­£å¸¸æƒ…å†µï¼‰
train(train_poly_features[:, :4], test_poly_features[:, :4], train_labels, test_labels)
ç‰¹å¾é€‰æ‹©ï¼šè¿™é‡Œä½¿ç”¨çš„æ˜¯å‰ä¸‰ä¸ªå¤šé¡¹å¼ç‰¹å¾ï¼ˆ[:, :4]ï¼‰ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬åœ¨æ„å»ºå¤šé¡¹å¼ç‰¹å¾æ—¶ï¼Œå¯èƒ½ä¼šé€šè¿‡æŸç§æ–¹å¼æ„é€ ä¸åŒé˜¶æ•°çš„ç‰¹å¾ï¼ˆæ¯”å¦‚äºŒæ¬¡ã€ä¸‰æ¬¡ã€å››æ¬¡ç­‰ï¼‰ï¼Œå¹¶é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„é˜¶æ•°è¿›è¡Œè®­ç»ƒã€‚
æ¨¡å‹å¤æ‚åº¦ï¼šä¸‰é˜¶å¤šé¡¹å¼é€šå¸¸åœ¨ä¸€å®šç¨‹åº¦ä¸Šèƒ½å¤Ÿæ•æ‰åˆ°æ•°æ®çš„éçº¿æ€§å…³ç³»ï¼Œä½†ä¸ä¼šè¿‡äºå¤æ‚ï¼Œå› æ­¤å®ƒé€šå¸¸èƒ½åœ¨æµ‹è¯•é›†ä¸Šæœ‰ä¸€ä¸ªè‰¯å¥½çš„æ³›åŒ–èƒ½åŠ›ã€‚

æ‹Ÿåˆçº¿æ€§æ¨¡å‹ï¼ˆæ¬ æ‹Ÿåˆï¼‰
train(train_poly_features[:, :2], test_poly_features[:, :2], train_labels, test_labels)
ç‰¹å¾é€‰æ‹©ï¼šè¿™é‡Œåªä½¿ç”¨äº†å‰ä¸¤ä¸ªå¤šé¡¹å¼ç‰¹å¾ï¼ˆ[:, :2]ï¼‰ã€‚è¿™äº›ç‰¹å¾é€šå¸¸æ˜¯ä½é˜¶çš„ï¼Œä»£è¡¨äº†æ•°æ®ä¸­çš„è¾ƒç®€å•æ¨¡å¼ã€‚
æ¨¡å‹å¤æ‚åº¦ï¼šç”±äºåªä½¿ç”¨äº†è¾ƒä½é˜¶çš„ç‰¹å¾ï¼Œæ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›æ¯”è¾ƒå¼±ï¼Œå¯èƒ½æ— æ³•å……åˆ†æ‹Ÿåˆè®­ç»ƒæ•°æ®ä¸­çš„å¤æ‚å…³ç³»ï¼Œå®¹æ˜“äº§ç”Ÿæ¬ æ‹Ÿåˆï¼ˆunderfittingï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæ¨¡å‹æ— æ³•æ•æ‰åˆ°æ•°æ®ä¸­çš„éçº¿æ€§å…³ç³»ï¼Œå› æ­¤åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šéƒ½ä¼šè¡¨ç°ä¸ä½³ã€‚

æ‹Ÿåˆ20é˜¶å¤šé¡¹å¼ï¼ˆè¿‡æ‹Ÿåˆï¼‰
train(train_poly_features, test_poly_features, train_labels, test_labels, num_epochs=1500)
ç‰¹å¾é€‰æ‹©ï¼šè¿™é‡Œä½¿ç”¨äº†æ‰€æœ‰çš„å¤šé¡¹å¼ç‰¹å¾ï¼ˆtrain_poly_features å’Œ test_poly_featuresï¼‰ï¼Œè¿™äº›ç‰¹å¾å¯èƒ½åŒ…æ‹¬éå¸¸é«˜é˜¶çš„é¡¹ï¼Œä¾‹å¦‚20é˜¶çš„å¤šé¡¹å¼ç‰¹å¾ã€‚
æ¨¡å‹å¤æ‚åº¦ï¼š20é˜¶çš„å¤šé¡¹å¼æ¨¡å‹éå¸¸å¤æ‚ï¼Œèƒ½å¤Ÿæ‹Ÿåˆè®­ç»ƒæ•°æ®ä¸­çš„å‡ ä¹æ‰€æœ‰ç»†èŠ‚ï¼ŒåŒ…æ‹¬å™ªå£°ã€‚è¿™ç§å¤æ‚çš„æ¨¡å‹å¾€å¾€ä¼šåœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•é›†ä¸Šè¡¨ç°è¾ƒå·®ï¼Œå› ä¸ºå®ƒè¿‡åº¦æ‹Ÿåˆäº†è®­ç»ƒé›†çš„ç»†èŠ‚ï¼Œæ— æ³•å¾ˆå¥½åœ°æ³›åŒ–åˆ°æ–°çš„æ•°æ®ï¼Œè¿™å°±æ˜¯è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰çš„è¡¨ç°ã€‚

æ€»ç»“ï¼šåŒºåˆ«å’Œè”ç³»
è¾“å…¥ç‰¹å¾çš„ç»´åº¦ä¸åŒï¼šåœ¨ä¸åŒçš„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè¾“å…¥ç‰¹å¾çš„ç»´åº¦ä¸åŒï¼Œå¯¼è‡´äº†æ¨¡å‹çš„å¤æ‚åº¦ä¹Ÿä¸åŒã€‚ä¸‰é˜¶å¤šé¡¹å¼æœ‰é€‚åº¦çš„å¤æ‚åº¦ï¼Œçº¿æ€§æ¨¡å‹ç‰¹å¾è¾ƒå°‘ï¼Œå¤æ‚åº¦è¾ƒä½ï¼Œ20é˜¶å¤šé¡¹å¼åˆ™æ‹¥æœ‰éå¸¸é«˜çš„å¤æ‚åº¦ã€‚
æ¨¡å‹çš„æ‹Ÿåˆæ•ˆæœä¸åŒï¼š
ä¸‰é˜¶å¤šé¡¹å¼æ¨¡å‹ä¸€èˆ¬èƒ½å¤Ÿåœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šå–å¾—è¾ƒå¥½çš„å¹³è¡¡ï¼Œé¿å…æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆã€‚
çº¿æ€§æ¨¡å‹ï¼ˆä½é˜¶ç‰¹å¾ï¼‰å¯èƒ½ä¼šæ¬ æ‹Ÿåˆï¼Œæ— æ³•æ•æ‰åˆ°æ•°æ®ä¸­çš„å¤æ‚å…³ç³»ã€‚
20é˜¶å¤šé¡¹å¼æ¨¡å‹å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œè™½ç„¶åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†æ— æ³•å¾ˆå¥½åœ°æ³›åŒ–åˆ°æµ‹è¯•é›†ï¼Œå¯¼è‡´æµ‹è¯•é›†çš„è¡¨ç°å¾ˆå·®ã€‚
è®­ç»ƒæ•°æ®çš„ç»´åº¦å’Œæ¨¡å‹è¡¨ç°çš„å…³ç³»
ç»´åº¦è¾ƒä½çš„ç‰¹å¾ï¼ˆå¦‚çº¿æ€§æ¨¡å‹çš„2é˜¶ç‰¹å¾ï¼‰å¾€å¾€å®¹æ˜“æ¬ æ‹Ÿåˆï¼Œå› ä¸ºå®ƒä»¬æ— æ³•æ•æ‰åˆ°æ•°æ®ä¸­çš„å¤æ‚å…³ç³»ã€‚
é€‚åº¦å¤æ‚çš„æ¨¡å‹ï¼ˆå¦‚ä¸‰é˜¶å¤šé¡¹å¼ç‰¹å¾ï¼‰é€šå¸¸èƒ½è¾ƒå¥½åœ°æ‹Ÿåˆæ•°æ®ï¼Œå–å¾—è¾ƒå¥½çš„æ•ˆæœã€‚
é«˜ç»´åº¦çš„ç‰¹å¾ï¼ˆå¦‚20é˜¶å¤šé¡¹å¼ç‰¹å¾ï¼‰é€šå¸¸å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå°¤å…¶æ˜¯å½“æ•°æ®é‡è¾ƒå°æ—¶ï¼Œæ¨¡å‹å®¹æ˜“å­¦ä¹ åˆ°è®­ç»ƒæ•°æ®ä¸­çš„å™ªå£°è€Œä¸æ˜¯æ•°æ®çš„çœŸå®è§„å¾‹ã€‚
'''
# ç»ƒä¹ 1ï¼šè¿™ä¸ªå¤šé¡¹å¼å›å½’é—®é¢˜å¯ä»¥å‡†ç¡®åœ°è§£å‡ºå—ï¼Ÿ
print("ç»ƒä¹ 1ï¼šä½¿ç”¨çº¿æ€§ä»£æ•°ç²¾ç¡®è§£å‡ºå¤šé¡¹å¼å›å½’é—®é¢˜")
print('-' * 50)


def solve_exactly(features, labels):    #solve_exactly(features, labels) æ˜¯ä½¿ç”¨æœ€å°äºŒä¹˜æ³•ç²¾ç¡®æ±‚è§£æƒé‡
    """ä½¿ç”¨æœ€å°äºŒä¹˜æ³•ç²¾ç¡®æ±‚è§£æƒé‡ã€‚

    å‚æ•°:
    features (Tensor): ç‰¹å¾çŸ©é˜µã€‚
    labels (Tensor): æ ‡ç­¾å‘é‡ã€‚

    è¿”å›:
    numpy.ndarray: ç²¾ç¡®æ±‚è§£çš„æƒé‡ã€‚
    """
    # å°†å¼ é‡è½¬æ¢ä¸º numpy æ•°ç»„
    X = features.numpy()
    y = labels.numpy()
    # è§£æ­£è§„æ–¹ç¨‹ X^T X w = X^T y
    w = np.linalg.lstsq(X, y, rcond=None)[0]    #np.linalg.lstsq(X, y, rcond=None)[0] æ˜¯è§£æ­£è§„æ–¹ç¨‹ X^T X w = X^T yï¼Œrcond=None æ˜¯ä¸ºäº†é˜²æ­¢è­¦å‘Šä¿¡æ¯ï¼Œrcond=None è¡¨ç¤ºä½¿ç”¨é»˜è®¤çš„å®¹å·®å€¼
    return w


# å¯¹ä¸‰é˜¶å¤šé¡¹å¼ç²¾ç¡®æ±‚è§£æƒé‡
w_exact = solve_exactly(train_poly_features[:, :4], train_labels)
print("ç²¾ç¡®è§£å‡ºçš„æƒé‡:", w_exact)
print("çœŸå®æƒé‡:", true_w[:4])
print('-' * 50)

# ç»ƒä¹ 2ï¼šè€ƒè™‘å¤šé¡¹å¼çš„æ¨¡å‹é€‰æ‹©
print("ç»ƒä¹ 2ï¼šå¤šé¡¹å¼çš„æ¨¡å‹é€‰æ‹©")
print('-' * 50)

degrees = list(range(1, max_degree + 1))
train_losses = []
test_losses = []

for degree in degrees:
    # é€‰æ‹©å½“å‰é˜¶æ•°çš„ç‰¹å¾
    train_feats = train_poly_features[:, :degree]
    test_feats = test_poly_features[:, :degree]

    # å®šä¹‰æ¨¡å‹
    net = nn.Linear(train_feats.shape[1], 1, bias=False)
    for param in net.parameters():
        nn.init.normal_(param, mean=0, std=0.01)

    # å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.SGD(net.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()

    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    batch_size = min(10, train_labels.shape[0])
    dataset = torch.utils.data.TensorDataset(train_feats, train_labels)
    train_iter = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    test_dataset = torch.utils.data.TensorDataset(test_feats, test_labels)
    test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    num_epochs = 100
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        net.train()
        for X, y in train_iter:
            optimizer.zero_grad()
            y_pred = net(X)
            loss = loss_fn(y_pred, y.view(-1, 1))
            loss.backward()
            optimizer.step()
    # è¯„ä¼°æŸå¤±
    net.eval()
    train_loss = evaluate_loss(net, train_iter, loss_fn)
    test_loss = evaluate_loss(net, test_iter, loss_fn)
    train_losses.append(train_loss)
    test_losses.append(test_loss)
    print(f'é˜¶æ•° {degree}, è®­ç»ƒæŸå¤±: {train_loss:.6f}, æµ‹è¯•æŸå¤±: {test_loss:.6f}')

# ç»˜åˆ¶è®­ç»ƒå’Œæµ‹è¯•æŸå¤±ä¸é˜¶æ•°çš„å…³ç³»å›¾
plt.figure()
plt.plot(degrees, train_losses, label='è®­ç»ƒæŸå¤±')
plt.plot(degrees, test_losses, label='æµ‹è¯•æŸå¤±')
plt.xlabel('å¤šé¡¹å¼çš„é˜¶æ•°')
plt.ylabel('æŸå¤±')
plt.legend()
plt.show()
print('-' * 50)

# ç»ƒä¹ 3ï¼šç”ŸæˆåŒæ ·çš„å›¾ï¼Œä½œä¸ºæ•°æ®é‡çš„å‡½æ•°
print("ç»ƒä¹ 3ï¼šç»˜åˆ¶æŸå¤±ä¸è®­ç»ƒæ•°æ®é‡çš„å…³ç³»å›¾")
print('-' * 50)

data_sizes = [5, 10, 20, 50, 70, 100]
train_losses = []
test_losses = []
degree = max_degree  # ä½¿ç”¨20é˜¶å¤šé¡¹å¼

for n_train_samples in data_sizes:
    # è·å–å½“å‰çš„æ•°æ®é‡
    train_feats = train_poly_features[:n_train_samples, :degree]
    train_labs = train_labels[:n_train_samples]
    test_feats = test_poly_features[:, :degree]

    # å®šä¹‰æ¨¡å‹
    net = nn.Linear(train_feats.shape[1], 1, bias=False)
    for param in net.parameters():
        nn.init.normal_(param, mean=0, std=0.01)

    # å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.SGD(net.parameters(), lr=0.01)
    loss_fn = nn.MSELoss()

    # å‡†å¤‡æ•°æ®åŠ è½½å™¨
    batch_size = min(10, train_labs.shape[0])
    dataset = torch.utils.data.TensorDataset(train_feats, train_labs)
    train_iter = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    test_dataset = torch.utils.data.TensorDataset(test_feats, test_labels)
    test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    num_epochs = 100
    # è®­ç»ƒå¾ªç¯
    for epoch in range(num_epochs):
        net.train()
        for X, y in train_iter:
            optimizer.zero_grad()
            y_pred = net(X)
            loss = loss_fn(y_pred, y.view(-1, 1))
            loss.backward()
            optimizer.step()
    # è¯„ä¼°æŸå¤±
    net.eval()
    train_loss = evaluate_loss(net, train_iter, loss_fn)
    test_loss = evaluate_loss(net, test_iter, loss_fn)
    train_losses.append(train_loss)
    test_losses.append(test_loss)
    print(f'è®­ç»ƒæ ·æœ¬æ•°: {n_train_samples}, è®­ç»ƒæŸå¤±: {train_loss:.6f}, æµ‹è¯•æŸå¤±: {test_loss:.6f}')

# ç»˜åˆ¶æŸå¤±ä¸æ•°æ®é‡çš„å…³ç³»å›¾
plt.figure()
plt.plot(data_sizes, train_losses, label='è®­ç»ƒæŸå¤±')
plt.plot(data_sizes, test_losses, label='æµ‹è¯•æŸå¤±')
plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°é‡')
plt.ylabel('æŸå¤±')
plt.legend()
plt.show()
print('-' * 50)

# ç»ƒä¹ 4ï¼šä½¿ç”¨æœªæ ‡å‡†åŒ–çš„å¤šé¡¹å¼ç‰¹å¾è¿›è¡Œè®­ç»ƒ
print("ç»ƒä¹ 4ï¼šä½¿ç”¨æœªæ ‡å‡†åŒ–çš„å¤šé¡¹å¼ç‰¹å¾è¿›è¡Œè®­ç»ƒ")
print('-' * 50)

# ç”Ÿæˆæœªæ ‡å‡†åŒ–çš„å¤šé¡¹å¼ç‰¹å¾
poly_features_no_norm = np.power(features.numpy(), np.arange(max_degree))
poly_features_no_norm = torch.tensor(poly_features_no_norm, dtype=torch.float32)

# åˆ’åˆ†æ•°æ®é›†
train_poly_features_no_norm = poly_features_no_norm[:n_train]
test_poly_features_no_norm = poly_features_no_norm[n_train:]

# è®­ç»ƒæ¨¡å‹
print("ä½¿ç”¨æœªæ ‡å‡†åŒ–çš„å¤šé¡¹å¼ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼ˆé˜¶æ•°=20ï¼‰")
train(train_poly_features_no_norm, test_poly_features_no_norm, train_labels, test_labels, num_epochs=100)
print('-' * 50)

# å¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ä»¥è§£å†³é—®é¢˜
print("å¯¹æœªæ ‡å‡†åŒ–çš„å¤šé¡¹å¼ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–")
mean = train_poly_features_no_norm.mean(dim=0, keepdim=True)
std = train_poly_features_no_norm.std(dim=0, keepdim=True)
train_poly_features_std = (train_poly_features_no_norm - mean) / std
test_poly_features_std = (test_poly_features_no_norm - mean) / std

print("ä½¿ç”¨æ ‡å‡†åŒ–åçš„æœªæ ‡å‡†åŒ–å¤šé¡¹å¼ç‰¹å¾è¿›è¡Œè®­ç»ƒï¼ˆé˜¶æ•°=20ï¼‰")
train(train_poly_features_std, test_poly_features_std, train_labels, test_labels, num_epochs=100)
print('-' * 50)

# åœ¨ç»ƒä¹ 4ä¸­ï¼Œæˆ‘ä»¬ç”Ÿæˆäº†æœªæ ‡å‡†åŒ–çš„å¤šé¡¹å¼ç‰¹å¾ï¼Œå³ç›´æ¥è®¡ç®—ğ‘¥^(ğ‘–)è€Œä¸é™¤ä»¥ğ‘–!
# ç”±äºç‰¹å¾ğ‘¥æ˜¯ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒä¸­é‡‡æ ·çš„ï¼Œå€¼å¯èƒ½åœ¨ -3 åˆ° 3 ä¹‹é—´ã€‚å½“æˆ‘ä»¬è®¡ç®—é«˜æ¬¡å¹‚ï¼ˆå¦‚ 20 æ¬¡æ–¹ï¼‰æ—¶ï¼Œè¿™äº›å€¼ä¼šå˜å¾—éå¸¸å¤§æˆ–éå¸¸å°ï¼ˆå¯¹äºè´Ÿæ•°è¿˜ä¼šæœ‰ç¬¦å·äº¤æ›¿ï¼‰ï¼Œå¯¼è‡´æ•°å€¼æº¢å‡ºæˆ–ä¸‹æº¢ã€‚
#
# ä¸¾ä¸ªä¾‹å­ï¼š
# å¦‚æœ ğ‘¥=3é‚£ä¹ˆ
# ğ‘¥^(20)=3^(20)â‰ˆ3.5Ã—10^(9)
# å¦‚æœğ‘¥=âˆ’3é‚£ä¹ˆ
# ğ‘¥^(20)=(âˆ’3)^(20)â‰ˆ3.5Ã—10^(9)
#
# å¦‚æ­¤å¤§çš„æ•°å€¼ä¼šå¯¼è‡´åœ¨è®¡ç®—è¿‡ç¨‹ä¸­å‡ºç°æº¢å‡ºï¼Œæƒé‡æ›´æ–°æ—¶äº§ç”Ÿ inf æˆ– nanï¼Œæœ€ç»ˆå¯¼è‡´æ¨¡å‹æ— æ³•å­¦ä¹ åˆ°æœ‰æ•ˆçš„å‚æ•°ã€‚
# å³ä½¿åœ¨ä¹‹åå¯¹è¿™äº›æœªæ ‡å‡†åŒ–çš„ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–ï¼ˆå‡å»å‡å€¼ï¼Œé™¤ä»¥æ ‡å‡†å·®ï¼‰ï¼Œç”±äºåŸå§‹æ•°æ®ä¸­å­˜åœ¨æç«¯å€¼ï¼Œæ ‡å‡†åŒ–åçš„æ•°æ®ä»ç„¶å¯èƒ½å­˜åœ¨æ•°å€¼é—®é¢˜ã€‚

# å‡½æ•°æ€»ç»“:
# - evaluate_loss(net, data_iter, loss_fn): åœ¨æ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹çš„æŸå¤±ã€‚
#   - net: ç¥ç»ç½‘ç»œæ¨¡å‹ã€‚
#   - data_iter: æ•°æ®åŠ è½½å™¨ã€‚
#   - loss_fn: æŸå¤±å‡½æ•°ã€‚
#
# - train(train_features, test_features, train_labels, test_labels, num_epochs=400):
#   è®­ç»ƒæ¨¡å‹å¹¶ç»˜åˆ¶è®­ç»ƒå’Œæµ‹è¯•æŸå¤±ã€‚
#   - train_features: è®­ç»ƒç‰¹å¾å¼ é‡ã€‚
#   - test_features: æµ‹è¯•ç‰¹å¾å¼ é‡ã€‚
#   - train_labels: è®­ç»ƒæ ‡ç­¾å¼ é‡ã€‚
#   - test_labels: æµ‹è¯•æ ‡ç­¾å¼ é‡ã€‚
#   - num_epochs: è®­ç»ƒçš„è½®æ•°ã€‚
#
# - solve_exactly(features, labels): ä½¿ç”¨æœ€å°äºŒä¹˜æ³•ç²¾ç¡®æ±‚è§£æƒé‡ã€‚
#   - features: ç‰¹å¾çŸ©é˜µå¼ é‡ã€‚
#   - labels: æ ‡ç­¾å¼ é‡ã€‚
