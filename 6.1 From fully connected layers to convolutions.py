# ============================================================
# 6.1 从全连接层到卷积
# ============================================================
print("6.1 从全连接层到卷积")

# ------------------------------------------------------------
# 本示例将分步骤演示从全连接层过渡到卷积运算的概念，展示 PyTorch 中相关操作的实现和用法。
# 将涵盖以下内容：
# 1. 全连接层与高维输入张量的参数爆炸问题演示
# 2. 利用卷积层减少参数、利用局部感受野与平移不变性进行特征提取的思路
# 3. 卷积层参数（卷积核大小、输入通道、输出通道、步幅、填充等）的含义与用法
# 4. 在控制台中清晰打印各函数示例、以及分割线分块展示
# 5. 在代码与注释中解释函数的必选参数和可选参数，并给出中文说明
# 6. 举例展示卷积层对图像的特征提取、多通道输入输出示例
# 7. 回答文档中给出的练习问题（在注释中说明）
#
# 注：本代码不依赖d2l包，如需数据生成与可视化，将直接使用PyTorch和matplotlib等。
#
# ------------------------------------------------------------

import torch
import torch.nn as nn
import time
import numpy as np
import matplotlib.pyplot as plt

# ------------------------------------------------------------
print("\n" + "="*60)
print("演示1: 全连接层在高维输入上的参数爆炸问题")
print("="*60)

# 假设我们有一个输入图像为 (C,H,W) = (3, 256, 256) 的彩色图像    #C-Channels, H-Height, W-Width
# 转换为向量后长度为 3*256*256 = 196,608 个特征。
# 若使用全连接层映射到 1000 个隐藏单元，则参数数量约为 196,608*1000 ≈ 1.96608e8 个参数。
C, H, W = 3, 256, 256
input_dim = C * H * W
hidden_units = 1000

fc_layer = nn.Linear(input_dim, hidden_units)
# 查看全连接层参数量
param_count_fc = sum(p.numel() for p in fc_layer.parameters())  #p.numel()返回参数数量,fc_layer.parameters()返回参数生成器
print(f"全连接层参数数量: {param_count_fc}")

# 这在实际中非常庞大，且对训练数据的需求和计算需求都非常高。

# ------------------------------------------------------------
print("\n" + "="*60)
print("演示2: 使用卷积层减少参数量")
print("="*60)

# 卷积层参数只依赖于卷积核大小和输入输出通道数，与输入的H,W无关。
# 例如：一个卷积核大小为 3x3、输入通道3、输出通道16的卷积层。
# 参数数量 = (3 * 3 * 输入通道数 * 输出通道数) + 输出通道数的偏置 = (3*3*3*16) + 16 = 432 + 16 =448
conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)  #Conv2d是一个用于创建二维卷积层的类。它接受多个参数来定义卷积层的属性。
param_count_conv = sum(p.numel() for p in conv_layer.parameters())
print(f"卷积层参数数量: {param_count_conv}")

# 与全连接层相比减少了几个数量级的参数量。

'''
卷积
卷积是一种数学运算，用于信号处理、图像处理等领域。在深度学习中，卷积操作特别用于提取图像或其他类型数据的特征。简单来说，卷积是通过一个小的权重矩阵（称为卷积核或滤波器）在输入数据上滑动，并对每个位置进行加权求和的过程。这个过程可以帮助模型学习到输入数据的空间层次结构。

卷积核
卷积核是一个小的矩阵，通常具有较小的尺寸（如3x3、5x5等）。在卷积操作中，这个核会在输入数据上滑动，并在每个位置计算加权和。卷积核的权重是在训练过程中学习的，以便提取有用的特征。

输入输出通道数
输入通道数：这指的是输入数据的深度或“颜色”通道数。例如，在图像处理中，RGB图像通常有3个通道（红、绿、蓝）。
输出通道数：这指的是卷积层输出的深度或“特征图”的数量。在深度学习中，每个特征图都可以看作是输入数据的一种特定表示或特征。

参数数量计算方法
对于一个卷积层，其参数数量主要由卷积核的权重和偏置组成。计算公式如下：
权重参数：每个卷积核都有一个与输入通道数相对应的深度，并且会在输出通道数的每个特征图上滑动。因此，每个卷积核的参数数量是 (卷积核高度 * 卷积核宽度 * 输入通道数)。由于有多个输出通道，所以总的权重参数数量是 (卷积核高度 * 卷积核宽度 * 输入通道数 * 输出通道数)。
偏置参数：每个输出通道都有一个偏置项。因此，偏置参数的数量等于输出通道数。
所以，总的参数数量是 (卷积核高度 * 卷积核宽度 * 输入通道数 * 输出通道数) + 输出通道数。

Conv2d
nn.Conv2d 是 PyTorch 中用于创建二维卷积层的类。它接受多个参数来定义卷积层的属性。
in_channels：输入通道数。
out_channels：输出通道数。
kernel_size：卷积核的大小。 输入3则是3x3的卷积核。
stride：卷积核在输入数据上滑动的步长。
padding：在输入数据的边缘添加的零填充的数量，以保持输出尺寸。

conv_layer = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)
这行代码创建了一个二维卷积层，其中：
in_channels=3 表示输入数据有3个通道（例如，RGB图像）。
out_channels=16 表示输出将有16个特征图或通道。
kernel_size=3 表示卷积核的大小为3x3。
stride=1 表示卷积核在输入数据上每次移动1个像素。
padding=1 表示在输入数据的边缘添加1层零填充，以保持输出尺寸与输入尺寸相同（在特定条件下）。
'''


# ------------------------------------------------------------
print("\n" + "="*60)
print("演示3: 卷积层的平移不变性与局部性原则")
print("="*60)

# 我们来创建一个简单的输入张量(单个图片)，演示卷积的平移不变性。
# 假设输入为 (N, C, H, W) = (1, 1, 5, 5), 单通道小图像 N-Batch, C-Channel, H-Height, W-Width
# 我们定义一个简单的卷积核来检测某些局部模式，比如中间的值。
input_tensor = torch.zeros((1,1,5,5))
input_tensor.fill_(2)
# # input_tensor = torch.full((1, 1, 5, 5), 2.0)
input_tensor[0,0,2,2] = 1.0  # 在中心位置放一个值为1的像素
input_tensor[0,0,2,3] = 3.0  # 在[2.3]放一个值为3的像素

# input_tensor[0,0] =
# tensor([[2., 2., 2., 2., 2.],
#         [2., 2., 2., 2., 2.],
#         [2., 2., 1., 3., 2.],
#         [2., 2., 2., 2., 2.],
#         [2., 2., 2., 2., 2.]])

# 定义卷积核为 3x3，大部分权重为0，中心为1，用来检测中心像素位置
#输出图像的大小将是 (输入高度 - 卷积核高度 + 1, 输入宽度 - 卷积核宽度 + 1)，即 (5 - 3 + 1, 5 - 3 + 1) = (3, 3)   #这里是stride=1
# 输出高度 = (输入高度 - 卷积核高度) // 步长 + 1
# 输出宽度 = (输入宽度 - 卷积核宽度) // 步长 + 1
simple_conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, bias=False)   #没有使用填充（padding=0），步长stride也是默认的1
with torch.no_grad():    # 不计算梯度，以减少计算量
    # 初始化卷积核，让中间的权重为1，其余为0
    # 卷积核形状： (out_channels, in_channels, kernel_height, kernel_width) = (1,1,3,3)
    simple_conv.weight.fill_(0.0)   #.fill_用于在原张量上直接修改值,(0.0)表示用0填充 simple_conv.weight.shape是(1,1,3,3)
    simple_conv.weight[0,0,1,1] = 1.0

    # simple_conv.weight =
    # tensor([[[[0., 0., 0.],
    #           [0., 1., 0.],
    #           [0., 0., 0.]]]])

output = simple_conv(input_tensor)  # 进行卷积运算
print("初始输入张量:\n", input_tensor[0,0])
print("卷积输出张量:\n", output[0,0])

# 我们将输入平移一下，看输出是否仍然识别到同样的模式
input_tensor_shifted = torch.zeros((1,1,5,5))
input_tensor_shifted[0,0,2,3] = 1.0 # 将"1"像素向右移动一列
output_shifted = simple_conv(input_tensor_shifted)
print("平移后的输入张量:\n", input_tensor_shifted[0,0])
print("平移后的卷积输出张量:\n", output_shifted[0,0])

# 可见输出的高值也随输入特征的移动而相应移动，体现了平移不变性。

# 卷积操作
# 步骤：
# 将卷积核放到输入张量的左上角，并在张量中滑动。
# 每个位置计算输入张量中被覆盖区域和卷积核的逐元素乘积之和。
# 输出张量中记录每次滑动的结果。
# 可视化计算
# 第一步：卷积核覆盖左上角 (输出位置 [0,0])
# 输入张量的覆盖部分：
# [[2. 2. 2.]
#  [2. 2. 2.]
#  [2. 2. 1.]]
# 逐元素乘积 + 求和：
# (2*0) + (2*0) + (2*0) +
# (2*0) + (2*1) + (2*0) +
# (2*0) + (2*0) + (1*0) = 2
# 输出位置 [0,0] 的值为 2。
# 第二步：卷积核向右滑动 (输出位置 [0,1])
# 输入张量的覆盖部分：
# [[2. 2. 2.]
#  [2. 2. 2.]
#  [2. 1. 3.]]
# 逐元素乘积 + 求和：
# (2*0) + (2*0) + (2*0) +
# (2*0) + (2*1) + (2*0) +
# (2*0) + (1*0) + (3*0) = 2
# 输出位置 [0,1] 的值为 2。
# ………………
# 第三步：继续滑动至中心 (输出位置 [1,1])
# 输入张量的覆盖部分：
# [[2. 2. 2.]
#  [2. 1. 3.]
#  [2. 2. 2.]]
# 逐元素乘积 + 求和：
# (2*0) + (2*0) + (2*0) +
# (2*0) + (1*1) + (3*0) +
# (2*0) + (2*0) + (2*0) = 1
# 输出位置 [1,1] 的值为 1。
# ………………
# 滑动完成后输出 最终的输出张量为：
# [[2. 2. 2.]
#  [2. 1. 3.]
#  [2. 2. 2.]]


# 填充（Padding）的作用
# padding 是在输入张量的边缘加上一些额外的像素（或元素），通常在卷积运算中使用，以控制输出张量的大小，或者保持输出张量与输入张量的尺寸相同。
# Padding=1 表示在每一边（上、下、左、右）各添加 1 层像素。
# 所以，如果原始输入张量的尺寸是 4x4，应用 padding=1 后，输入张量的尺寸会变为 6x6。
#
# 举个例子：
# 假设我们有一个 4x4 的输入张量：
# Input Tensor (4x4):
# [[1., 2., 3., 4.],
#  [5., 6., 7., 8.],
#  [9., 10., 11., 12.],
#  [13., 14., 15., 16.]]
# 当我们应用 padding=1 时，新的尺寸是 6x6，我们在输入张量的每个边（上、下、左、右）加上一行/列的零。结果是：
# Input Tensor with padding=1 (6x6):
# [[0., 0., 0., 0., 0., 0.],
#  [0., 1., 2., 3., 4., 0.],
#  [0., 5., 6., 7., 8., 0.],
#  [0., 9., 10., 11., 12., 0.],
#  [0., 13., 14., 15., 16., 0.],
#  [0., 0., 0., 0., 0., 0.]]
# 计算输出张量的尺寸
# 卷积运算后输出张量的尺寸计算公式是：
# 输出(h/w) = (输入(h/w) - 卷积核(h/w)) // 步长 + 1
# 其中：
# 输入尺寸是经过填充后的尺寸
# 卷积核大小为 2x2
# 步长为 1
# 对于填充为 1、步长为 1 的情况：
#
# 输入张量的尺寸为 6x6
# 卷积核的尺寸为 2x2
# 步长为 1

'''
当 (input_size - kernel_size) / stride 计算结果不是整数时，卷积操作依然会进行，但是卷积层的输出尺寸会向下取整（floor）。也就是说，输出尺寸将是整数，即使 (input_size - kernel_size) / stride 的结果是小数。
我们可以通过 向下取整 的方式处理这种情况，确保输出尺寸是一个整数。

例子 3：小数的情况
假设输入张量大小是 8x8，卷积核大小是 3x3，步长为 3。我们来看输出尺寸：
[(8 - 3) / 3 + 1] = 2.6667
即使 (8 - 3) / 3 = 1.6667，输出尺寸依然是 2x2，因为 floor(2.6667) 取整后为 2。
'''

# ------------------------------------------------------------
print("\n" + "="*60)
print("演示4: 多通道输入与输出的卷积操作")
print("="*60)

# 当输入有多通道时，卷积核会对每个通道分别加权，再求和产生输出通道特征。
# 演示：输入通道数=3, 输出通道数=2, 卷积核=3x3
multi_channel_conv = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, padding=1, bias=True)

# 随机输入(N=1张图片, C=3通道, H=4, W=4)
multi_input = torch.randn(1, 3, 4, 4)
multi_output = multi_channel_conv(multi_input)

print("多通道输入张量形状:", multi_input.shape)
print("卷积后多通道输出张量形状:", multi_output.shape)  #宽=(4-3+2*1)/1 + 1 = 4, 高=(4-3+2*1)/1 + 1 = 4
#宽=(输入宽-卷积核宽+2*填充)/步长 + 1, 高=(输入高-卷积核高+2*填充)/步长 + 1
# 输出形状为 (1, 2, 4, 4), 即2个输出通道的特征图


print("输出通道1的滤波器权重:")
print(multi_channel_conv.weight[0])  # 第一个输出通道的权重
print("\n输出通道1的偏置:")
print(multi_channel_conv.bias[0])

print("\n输出通道2的滤波器权重:")
print(multi_channel_conv.weight[1])  # 第二个输出通道的权重
print("\n输出通道2的偏置:")
print(multi_channel_conv.bias[1])
'''
卷积滤波器的初始化与多输出通道的意义
在卷积神经网络（CNN）中，卷积滤波器（或称为卷积核）的初始化和多输出通道的设置对于模型的表现至关重要。让我们详细探讨一下您提到的两个关键问题：
两个输出通道的滤波器是如何被初始化的？
原始数据相同而使用两个滤波器（都是随机生成权重的）有什么意义？

1. 卷积滤波器的初始化
默认初始化方式
在PyTorch中，当您创建一个 nn.Conv2d 层时，卷积层的权重和偏置会自动进行初始化。具体来说：
权重（Weights）：
Kaiming（He）初始化：对于大多数激活函数（如ReLU），PyTorch使用Kaiming初始化方法。这种方法根据输入通道数和卷积核的尺寸来初始化权重，以保持前向传播过程中信号的方差稳定。
偏置（Biases）：
零初始化：默认情况下，偏置项初始化为零。
代码示例：查看初始化的权重

让我们通过代码示例来查看 multi_channel_conv 的初始化权重和偏置：
# 定义卷积层
multi_channel_conv = nn.Conv2d(in_channels=3, out_channels=2, kernel_size=3, padding=1, bias=True)
# 查看权重和偏置
print("输出通道1的滤波器权重:")
print(multi_channel_conv.weight[0])  # 第一个输出通道的权重
print("\n输出通道1的偏置:")
print(multi_channel_conv.bias[0])
print("\n输出通道2的滤波器权重:")
print(multi_channel_conv.weight[1])  # 第二个输出通道的权重
print("\n输出通道2的偏置:")
print(multi_channel_conv.bias[1])
输出示例（注意：每次运行时权重会不同，因为它们是随机初始化的）：
输出通道1的滤波器权重:
tensor([[[ 0.1234, -0.5678,  0.9101],
         [ 0.1121, -0.3141,  0.5161],
         [-0.7181,  0.9202, -0.2233]],

        [[ 0.3344, -0.4455,  0.5566],
         [ 0.6677, -0.7788,  0.8899],
         [-0.9900,  1.1011, -1.2121]],

        [[-0.3131,  0.4242, -0.5353],
         [ 0.6464, -0.7575,  0.8686],
         [-0.9797,  1.0909, -1.2020]]])
输出通道1的偏置:
tensor(0.1234)
输出通道2的滤波器权重:
tensor([[[ 0.5678, -0.6789,  0.7890],
         [ 0.8901, -0.9012,  1.0123],
         [-1.1234,  1.2345, -1.3456]],

        [[ 0.4567, -0.5678,  0.6789],
         [ 0.7890, -0.8901,  1.0012],
         [-1.1123,  1.2234, -1.3345]],

        [[-0.4455,  0.5566, -0.6677],
         [ 0.7788, -0.8899,  1.0000],
         [-1.1111,  1.2222, -1.3333]]])
输出通道2的偏置:
tensor(-0.5678)

重点要点
独立初始化：每个输出通道的滤波器权重都是独立初始化的，即使它们对应相同的输入通道数。这样，每个输出通道可以学习到不同的特征。
随机性：权重的随机初始化有助于打破对称性，使得不同的滤波器在训练过程中能够学习到不同的特征表示。

2. 多输出通道的意义
多输出通道的作用
在卷积层中使用多个输出通道（即多个滤波器）有以下几个主要目的：
捕捉多样化的特征：
每个滤波器可以学习到输入数据的不同特征。例如，在图像处理中，一个滤波器可能学习到边缘检测，另一个可能学习到纹理或颜色模式。
增强表达能力：
多个输出通道允许模型在同一层中表达更多的信息，从而提高模型的表达能力和复杂性。
信息的多样化传递：
多通道输出可以为后续的网络层提供更丰富的信息，支持更复杂的特征组合和层级结构。
为什么使用不同的随机滤波器？
避免冗余：
如果所有输出通道使用相同的滤波器，那么它们将学习到相同的特征，导致信息的冗余。通过不同的随机初始化，滤波器被迫探索不同的特征空间。
促进学习多样化特征：
不同的初始权重使得每个滤波器在训练初期关注于不同的特征，从而在整个训练过程中能够捕捉到更多样化和有用的特征。

举例说明
假设我们有一个简单的彩色图像输入（3个通道：红、绿、蓝），通过一个卷积层产生2个输出通道：
输出通道1：
可能学习到检测图像中的边缘，例如通过学习到类似于Sobel滤波器的权重。
输出通道2：
可能学习到检测图像中的纹理或颜色变化，例如通过学习到颜色梯度或其他纹理模式。
这样的多样化特征捕捉有助于网络在后续层中进行更复杂的模式识别和分类任务。
'''

'''
基本概念回顾
在卷积神经网络（CNN）中，多通道卷积的基本操作如下：
输入张量：假设输入张量的形状为 (N, C_in, H, W)，其中：
N 是批量大小（Batch Size）
C_in 是输入通道数
H 和 W 是输入的高度和宽度
卷积层：一个卷积层有多个滤波器（或称为核），每个滤波器用于生成一个输出通道。假设卷积层的参数为：
C_out 是输出通道数
每个滤波器的形状为 (C_in, K_H, K_W)，其中 K_H 和 K_W 是核的高度和宽度
输出张量：输出张量的形状为 (N, C_out, H_out, W_out)，其中 H_out 和 W_out 由卷积的参数决定。

具体示例
假设我们有以下参数：
输入张量：
批量大小 N = 1
输入通道数 C_in = 3
输入高度和宽度 H = W = 4
卷积层：
输出通道数 C_out = 2
核大小 K_H = K_W = 3
填充 padding = 1
步长 stride = 1
输入张量形状为 (1, 3, 4, 4)，输出张量形状为 (1, 2, 4, 4)。

步骤解析
让我们详细分解一个输出通道是如何从多个输入通道生成的。
1. 输入张量
假设输入张量 multi_input 如下（为了简化，我们只展示数值的一部分）：
Channel 1:
[[a, b, c, d],
 [e, f, g, h],
 [i, j, k, l],
 [m, n, o, p]]
Channel 2:
[[q, r, s, t],
 [u, v, w, x],
 [y, z, a1, b1],
 [c1, d1, e1, f1]]
Channel 3:
[[g1, h1, i1, j1],
 [k1, l1, m1, n1],
 [o1, p1, q1, r1],
 [s1, t1, u1, v1]]
2. 卷积滤波器
对于 C_out = 2，我们有2个输出通道，每个输出通道有3个3x3的滤波器，分别对应3个输入通道。
输出通道1的滤波器：
滤波器1-1（对应输入通道1）：
[[w11, w12, w13],
 [w14, w15, w16],
 [w17, w18, w19]]
滤波器1-2（对应输入通道2）：
[[w21, w22, w23],
 [w24, w25, w26],
 [w27, w28, w29]]
滤波器1-3（对应输入通道3）：
[[w31, w32, w33],
 [w34, w35, w36],
 [w37, w38, w39]]
偏置 b1
输出通道2的滤波器：
滤波器2-1（对应输入通道1）：
[[w41, w42, w43],
 [w44, w45, w46],
 [w47, w48, w49]]
滤波器2-2（对应输入通道2）：
[[w51, w52, w53],
 [w54, w55, w56],
 [w57, w58, w59]]
滤波器2-3（对应输入通道3）：
[[w61, w62, w63],
 [w64, w65, w66],
 [w67, w68, w69]]
偏置 b2
3. 计算输出通道1的某个位置
假设我们计算输出通道1在位置 (1,1) 的值。由于使用了 padding=1 和 stride=1，输入和输出的空间维度保持一致（4x4）。
步骤：
滑动窗口：选择输入张量在每个输入通道上的3x3窗口，中心对准 (1,1)。
逐通道卷积：
对输入通道1的窗口与滤波器1-1进行元素乘法并求和。
对输入通道2的窗口与滤波器1-2进行元素乘法并求和。
对输入通道3的窗口与滤波器1-3进行元素乘法并求和。
汇总：
将上述三个结果相加，再加上偏置 b1，得到输出通道1在 (1,1) 的值。
数学表示：
             3  3   3
Output1(1,1)=∑  ∑   ∑  (Input𝑐(𝑖,𝑗)×Filter1,𝑐(𝑖,𝑗))+𝑏1
            𝑐=1 𝑖=1 𝑗=1
总而言之，output是由所有输入通道的卷积结果加权求和得到的。每个输入特征都会有对应的滤波器，每个滤波器对应一个输入通道。
4. 计算输出通道2的某个位置
同样地，输出通道2的计算方法类似，只是使用对应的滤波器2-1、2-2、2-3和偏置 b2。
完整的输出生成过程
对于每个输出通道和每个空间位置，重复上述步骤。具体流程如下：
初始化输出张量 multi_output，形状为 (1, 2, 4, 4)。
对于每个输出通道 (C_out = 2)：
对于每个输入通道 (C_in = 3)：
应用对应的3x3滤波器，对输入通道进行卷积操作。
汇总所有输入通道的卷积结果，添加偏置，得到当前输出通道的一个特征图。
重复以上步骤，直到生成所有输出通道的特征图。
可视化示意图
输入张量 (C_in=3):
+----------+   +----------+   +----------+
| Channel1 |   | Channel2 |   | Channel3 |
+----------+   +----------+   +----------+

卷积滤波器 (C_out=2):
+-------------------+   +-------------------+
| Filter1-1,1-2,1-3 |   | Filter2-1,2-2,2-3 |
+-------------------+   +-------------------+

输出张量 (C_out=2):
+----------+   +----------+
| Output1  |   | Output2  |
+----------+   +----------+
Filter1-1,1-2,1-3：用于生成输出通道1的三个输入通道的滤波器。
Filter2-1,2-2,2-3：用于生成输出通道2的三个输入通道的滤波器。
每个输出通道通过对所有输入通道应用对应的滤波器并汇总得到。
总结
通过上述步骤和示意，你可以看到：
每个输出通道 都是通过 所有输入通道 的卷积结果加权求和得到的。



'''

# ------------------------------------------------------------
print("\n" + "="*60)
print("演示5: 卷积层的参数及用法介绍")
print("="*60)

# torch.nn.Conv2d函数参数介绍：
# 必选参数：
#   in_channels(int): 输入通道数，如RGB图像为3通道
#   out_channels(int): 输出通道数，即卷积产生的特征图数量
#   kernel_size(int或tuple): 卷积核大小，如3或(3,3)
# 输入3：这表示卷积核是一个正方形，其高度和宽度都是3。在这种情况下，kernel_size会被自动扩展为一个元组(3, 3)。
# 输入(3,3)：这直接指定了卷积核的高度为3，宽度也为3。
#
# 可选参数（常用）：
#   stride(int或tuple, 默认1): 卷积核移动步幅，每次滑动多少像素
#   padding(int或tuple, 默认0): 在输入图像周围增加像素填充，控制输出大小
#   dilation(int或tuple, 默认1): 卷积核元素之间的间隔
#   groups(int, 默认1): 分组卷积, 控制输入输出之间的连接模式
#   bias(bool, 默认True): 是否使用偏置项
#
# 示例: Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1)
# 意义：输入3通道特征图，使用3x3卷积核，步幅为2，输出16通道特征图，
#       并在输入每边padding为1，从而控制输出空间尺寸。
#
# 实例演示：
conv_example = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=2, padding=1, bias=True)
example_input = torch.randn(1,3,64,64)
example_output = conv_example(example_input)
print("Conv2d函数示例输出形状:", example_output.shape)
#宽=(输入宽-核宽+2*padding)/步幅+1=(64-3+2*1)/2+1=32, 高=(输入高-核高+2*padding)/步幅+1=(64-3+2*1)/2+1=32

# ------------------------------------------------------------
print("\n" + "="*60)
print("演示6: 将卷积等同于特殊情形下的全连接层（Δ=0情形）")
#detla=0--这里的Δ(delta）衡量的标准是卷积核的尺寸，当卷积核大小为1x1时，卷积操作仅在每个像素位置上对通道进行线性变换，而不考虑相邻像素。
print("="*60)

# 当卷积核大小为1x1时，相当于在每个像素位置上对通道进行线性变换。
# 这就像对每个像素点的通道特征进行一个全连接层映射，不考虑相邻像素。
#
# 创建一个1x1卷积来验证此思想：
one_by_one_conv = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=1)
# 输入为 (N,C,H,W)，输出的 (N,5,H,W) 中，每个像素的5维向量是输入3维像素向量的线性变换。
one_by_one_input = torch.randn(1,3,10,10)
one_by_one_output = one_by_one_conv(one_by_one_input)
print("1x1卷积示例输出形状:", one_by_one_output.shape)
# 等价于对于每个像素位置，有一个线性变换 (3->5)，不依赖空间邻域，从而可视为为每个通道独立进行全连接。


# ------------------------------------------------------------
print("\n" + "="*60)
print("文档中练习问题的说明与回答（在注释中给出）")
print("="*60)

# 练习问题回答：
# 1. 假设卷积层覆盖区域Δ=0，即卷积核大小为1x1的情况。此时卷积层对每个空间位置的输出仅依赖该位置的输入通道值。
#    因此，对每个像素点的通道特征做线性组合即相当于一个全连接层（针对通道维度），不考虑空间邻域。这与上面one_by_one_conv的例子相同。
#
# 2. 为什么平移不变性可能不是好主意？
#    在某些任务中（如语义信息可能与绝对位置相关），简单的平移不变性会丢失全局定位信息。例如，在图像中有些物体只在特定区域出现，
#    强制平移不变性会让模型无法利用这些全局位置信号。
#
# 3. 当从图像边界像素获取隐藏表示时，需要考虑填充(padding)问题，以确保卷积核有足够的像素计算特征，
#    否则边缘像素的信息可能比中心像素处理不公平。例如使用padding=1可以在边界增加虚拟像素（通常为0）以保持输出尺寸和信息对称。
#
# 4. 类似的音频卷积层架构是使用1D卷积（nn.Conv1d）来处理时间序列数据，每个卷积核在时间轴上一维展开，
#    从而利用局部性和平移不变性在音频信号中提取特征。
#
# 5. 对文本数据同样适用卷积层（nn.Conv1d），将词向量序列当作时间序列，利用卷积核在词序列上滑动来提取n-gram特征。
#
# 6. 证明 f*g = g*f（卷积的交换律）：
#    卷积定义：(f*g)(i,j) = sum_a sum_b f(a,b)*g(i-a,j-b)，
#    若交换f和g的位置，(g*f)(i,j) = sum_a sum_b g(a,b)*f(i-a,j-b)。
#    通过变量替换(a' = i-a, b' = j-b)可得到两者表达式一致，从而 f*g = g*f。
#    此为数学性质，这里不在代码中演示。


# ------------------------------------------------------------
print("\n" + "="*60)
print("总结本代码示例使用的函数与参数介绍")
print("="*60)
# 在本代码示例中使用的主要函数与模块：
# 1. nn.Linear(in_features, out_features, bias=True): 全连接层
#    参数：
#      in_features(int): 输入特征维度
#      out_features(int): 输出特征维度
#      bias(bool): 是否使用偏置项
#    功能：实现 y = xW^T + b 的线性映射。
#
# 2. nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True):
#    参数：
#      in_channels(int): 输入通道数
#      out_channels(int): 输出通道数
#      kernel_size(int或tuple): 卷积核高宽
#      stride(int或tuple): 步幅
#      padding(int或tuple): 填充大小
#      bias(bool): 是否使用偏置
#    功能：对输入图像进行2D卷积操作，从局部区域提取特征并生成特征图。
#
# 3. torch.randn(*size): 生成正态分布的随机张量
# 4. tensor.fill_(value): 用指定的值填充张量
#
# 调用示例已在上方代码中给出，通过print函数在控制台输出说明。
#
# 整个示例演示了从全连接层到卷积层的过渡，展现了卷积的参数高效性、平移不变性、局部性、多通道特性，
# 以及将1x1卷积等价为通道维全连接的特例，帮助理解卷积神经网络的构造及优势。
#
# 通过上述代码和注释，可以更全面地理解卷积层在深度学习中的使用场景与原理。



'''
Δ 表示卷积操作的局部感受野的大小（即每个卷积核作用的局部区域的半径）。具体来说，Δ 决定了卷积核在输入图像或特征图上滑动时，每次覆盖的区域的大小。

Δ 的含义：
局部区域的大小：Δ 定义了卷积核作用于输入的范围。对于每个元素 [i,j]，卷积核会覆盖一个以 (i,j) 为中心的局部区域，范围从 [i−Δ,j−Δ] 到 [i+Δ,j+Δ]。
卷积核大小：卷积核的尺寸通常是 
(2Δ+1)×(2Δ+1)，即 Δ 为 1 时，卷积核的大小为 3×3（涵盖了从 (i−1,j−1) 到 (i+1,j+1) 的9个元素）。
当 Δ=1 时：
局部感受野是以当前元素为中心，覆盖其周围的 3x3 区域。换句话说，卷积核的大小为 3×3。
每个卷积操作将会使用一个 3×3 的卷积核与输入的局部区域进行卷积操作。
解释：
在公式中：
            Δ    Δ
[𝐻]𝑖,𝑗 = 𝑢+  ∑    ∑   [𝑉]𝑎,𝑏 [𝑋]𝑖+𝑎,𝑗+𝑏
           𝑎=−Δ b=−Δ
[𝑉]𝑎,𝑏是卷积核的元素。
[𝑋]𝑖+𝑎,𝑗+𝑏是输入图像或特征图中当前位置的值。

当 Δ=1 时，卷积核的形状为 3×3，即 a 和 b 都从 −1 到 1（包含 (-1, 0, 1\）三个值）。
所以，如果 Δ=1，卷积核的形状是 3×3，卷积核的元素将应用于输入图像的 3×3 区域进行卷积计算。

当 Δ=0 时：
这意味着卷积操作仅仅作用于当前像素本身，即没有上下文的局部区域。这时，卷积核的形状变为 1×1，卷积操作实际上变成了一个全连接层。每个输入特征图的单一元素与卷积核的单一元素相乘，因此每组通道将独立实现一个全连接层。

总结：
Δ 是卷积操作的局部感受野的半径。
当 Δ=1 时，卷积核的形状是 3×3。

'''