print("4.1. å¤šå±‚æ„ŸçŸ¥æœº")
# å¯¼å…¥å¿…è¦çš„åº“
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import numpy as np
from matplotlib import rcParams
# é…ç½®ä¸­æ–‡å­—ä½“    å¾®è½¯é›…é»‘
rcParams['font.family'] = 'Microsoft YaHei'
# -------------------------------
print("ReLUæ¿€æ´»å‡½æ•°ç¤ºä¾‹")
# -------------------------------

# å®šä¹‰è¾“å…¥èŒƒå›´ï¼Œè®¾ç½®requires_grad=Trueä»¥ä¾¿è®¡ç®—æ¢¯åº¦
x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)

# ä½¿ç”¨torch.nn.functionalä¸­çš„reluå‡½æ•°è®¡ç®—ReLUæ¿€æ´»å‡½æ•°
# reluå‡½æ•°çš„å®šä¹‰ä¸ºï¼šrelu(input, inplace=False)
# å‚æ•°ï¼š
# - inputï¼šè¾“å…¥å¼ é‡
# - inplaceï¼šæ˜¯å¦è¿›è¡ŒåŽŸåœ°æ“ä½œï¼Œé»˜è®¤ä¸ºFalse
y = F.relu(x)     # è®¡ç®—ReLUæ¿€æ´»å‡½æ•°

# ç»˜åˆ¶ReLUæ¿€æ´»å‡½æ•°çš„å›¾åƒ
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach().numpy(), y.detach().numpy())    #detach()ï¼šè¿”å›žä¸€ä¸ªæ–°çš„å¼ é‡ï¼Œä¸Žå½“å‰å¼ é‡å…±äº«æ•°æ®ï¼Œä½†ä¸å…·æœ‰æ¢¯åº¦ä¿¡æ¯
plt.title('ReLUæ¿€æ´»å‡½æ•°')
plt.xlabel('x')
plt.ylabel('ReLU(x)')
plt.show()

# è®¡ç®—ReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°
y.backward(torch.ones_like(x))
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach().numpy(), x.grad.detach().numpy())
plt.title('ReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°')
plt.xlabel('x')
plt.ylabel('grad of ReLU(x)')
plt.show()

# æ¸…é™¤æ¢¯åº¦
x.grad.zero_()

# -------------------------------
print("Sigmoidæ¿€æ´»å‡½æ•°ç¤ºä¾‹")
# -------------------------------

# è®¡ç®—Sigmoidæ¿€æ´»å‡½æ•°çš„è¾“å‡º
# torch.sigmoidå‡½æ•°çš„å®šä¹‰ä¸ºï¼šsigmoid(input)
# å‚æ•°ï¼š
# - inputï¼šè¾“å…¥å¼ é‡
y = torch.sigmoid(x)

# ç»˜åˆ¶Sigmoidæ¿€æ´»å‡½æ•°çš„å›¾åƒ
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach().numpy(), y.detach().numpy())
plt.title('Sigmoidæ¿€æ´»å‡½æ•°')
plt.xlabel('x')
plt.ylabel('Sigmoid(x)')
plt.show()

# è®¡ç®—Sigmoidæ¿€æ´»å‡½æ•°çš„å¯¼æ•°
x.grad.zero_()  # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦
y.backward(torch.ones_like(x))
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach().numpy(), x.grad.detach().numpy())
plt.title('Sigmoidæ¿€æ´»å‡½æ•°çš„å¯¼æ•°')
plt.xlabel('x')
plt.ylabel('grad of Sigmoid(x)')
plt.show()

# -------------------------------
print("Tanhæ¿€æ´»å‡½æ•°ç¤ºä¾‹")
# -------------------------------

# è®¡ç®—Tanhæ¿€æ´»å‡½æ•°çš„è¾“å‡º
# torch.tanhå‡½æ•°çš„å®šä¹‰ä¸ºï¼štanh(input)
# å‚æ•°ï¼š
# - inputï¼šè¾“å…¥å¼ é‡
y = torch.tanh(x)

# ç»˜åˆ¶Tanhæ¿€æ´»å‡½æ•°çš„å›¾åƒ
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach().numpy(), y.detach().numpy())
plt.title('Tanhæ¿€æ´»å‡½æ•°')
plt.xlabel('x')
plt.ylabel('Tanh(x)')
plt.show()

# è®¡ç®—Tanhæ¿€æ´»å‡½æ•°çš„å¯¼æ•°
x.grad.zero_()  # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦
y.backward(torch.ones_like(x))
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach().numpy(), x.grad.detach().numpy())
plt.title('Tanhæ¿€æ´»å‡½æ•°çš„å¯¼æ•°')
plt.xlabel('x')
plt.ylabel('grad of Tanh(x)')
plt.show()

# -------------------------------
print("PReLUæ¿€æ´»å‡½æ•°ç¤ºä¾‹")
# -------------------------------

# å®šä¹‰PReLUæ¿€æ´»å‡½æ•°
# ä½¿ç”¨nn.PReLUæ¨¡å—
# PReLU(num_parameters=1, init=0.25)
# å‚æ•°ï¼š
# - num_parametersï¼šÎ±çš„æ•°é‡ï¼Œå¯ä»¥æ˜¯1ï¼ˆæ‰€æœ‰é€šé“å…±äº«ä¸€ä¸ªÎ±ï¼‰æˆ–è€…è¾“å…¥é€šé“æ•°
# - initï¼šÎ±çš„åˆå§‹å€¼ï¼Œé»˜è®¤ä¸º0.25

prelu = nn.PReLU(num_parameters=1, init=0.25)

# è®¡ç®—PReLUæ¿€æ´»å‡½æ•°çš„è¾“å‡º
y = prelu(x)

# ç»˜åˆ¶PReLUæ¿€æ´»å‡½æ•°çš„å›¾åƒ
plt.figure(figsize=(5, 2.5))    # ç»˜åˆ¶PReLUæ¿€æ´»å‡½æ•°çš„å›¾åƒ
plt.plot(x.detach().numpy(), y.detach().numpy())
plt.title('PReLUæ¿€æ´»å‡½æ•°')
plt.xlabel('x')
plt.ylabel('PReLU(x)')
plt.show()

# è®¡ç®—PReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°
x.grad.zero_()  # æ¸…é™¤ä¹‹å‰çš„æ¢¯åº¦
y.backward(torch.ones_like(x))
plt.figure(figsize=(5, 2.5))
plt.plot(x.detach().numpy(), x.grad.detach().numpy())
plt.title('PReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°')
plt.xlabel('x')
plt.ylabel('grad of PReLU(x)')
plt.show()

# x.grad.zero_()ï¼šè¿™æ˜¯æ¸…ç©º x çš„æ¢¯åº¦ï¼Œé˜²æ­¢ä¸Šæ¬¡è®¡ç®—çš„æ¢¯åº¦å½±å“åˆ°è¿™æ¬¡çš„è®¡ç®—ã€‚
# y.backward(torch.ones_like(x))ï¼šæ‰§è¡Œåå‘ä¼ æ’­ï¼Œè®¡ç®— y å¯¹ x çš„æ¢¯åº¦ã€‚torch.ones_like(x) æ˜¯åå‘ä¼ æ’­æ—¶ä½¿ç”¨çš„æ¢¯åº¦ï¼ˆé€šå¸¸æ˜¯æŸå¤±å‡½æ•°çš„æ¢¯åº¦ï¼‰ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªä¸Ž x ç›¸åŒå½¢çŠ¶çš„å…¨1å¼ é‡æ¥è¿›è¡Œåå‘ä¼ æ’­ï¼Œç›®çš„æ˜¯è®¡ç®—æ¯ä¸ª x å¯¹åº”çš„ PReLU æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ã€‚
# plt.plot(x.detach().numpy(), x.grad.detach().numpy())ï¼šç»˜åˆ¶ x å’Œå…¶æ¢¯åº¦ä¹‹é—´çš„å…³ç³»ï¼Œå³ PReLU æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ã€‚
#
# å½“ ð‘¥>0 æ—¶ï¼ŒPReLU(x)=x
#
# å½“ xâ‰¤0 æ—¶ï¼ŒPReLU(x)=Î±xï¼Œå…¶ä¸­ ð›¼ æ˜¯ä¸€ä¸ªå­¦ä¹ çš„å‚æ•°ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå°çš„æ­£å€¼ã€‚
#
# å¯¼æ•°ï¼ˆæ¢¯åº¦ï¼‰ä¸ºï¼šx>0 æ—¶ï¼Œå¯¼æ•°æ˜¯ 1
# å½“ xâ‰¤0 æ—¶ï¼Œå¯¼æ•°æ˜¯ ð›¼
#åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼šy.backward(torch.ones_like(x)) ä¼šè®¡ç®— y å¯¹ x çš„æ¢¯åº¦ã€‚ç”±äºŽ y æ˜¯é€šè¿‡ PReLU æ¿€æ´»å‡½æ•°è®¡ç®—å¾—åˆ°çš„ï¼Œæ‰€ä»¥åå‘ä¼ æ’­åŽï¼Œx.grad å°†åŒ…å« PReLU æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ã€‚
# ç»˜åˆ¶æ¢¯åº¦ï¼šé€šè¿‡ x.grad.detach().numpy() èŽ·å–è®¡ç®—å‡ºçš„æ¢¯åº¦ï¼Œå¹¶ä¸Ž x.detach().numpy() ä¸€èµ·ç»˜åˆ¶ï¼Œè¿™æ ·èƒ½å±•ç¤ºå‡º PReLU æ¿€æ´»å‡½æ•°åœ¨ä¸åŒè¾“å…¥å€¼ä¸‹çš„å¯¼æ•°ã€‚
# æ‰“å°Î±çš„å€¼
print("PReLUçš„å‚æ•°Î±ï¼š", prelu.weight.item())

# -------------------------------
print("å¤šå±‚æ„ŸçŸ¥æœºç¤ºä¾‹")


# -------------------------------

# å®šä¹‰ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºæ¨¡åž‹ï¼ŒåŒ…å«å¤šä¸ªéšè—å±‚å’Œå¯é€‰çš„æ¿€æ´»å‡½æ•°

class MLP(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size, activation_function=nn.ReLU()):
        """
        åˆå§‹åŒ–å¤šå±‚æ„ŸçŸ¥æœºæ¨¡åž‹
        å‚æ•°ï¼š
        - input_sizeï¼šè¾“å…¥å±‚å¤§å°ï¼ˆç‰¹å¾æ•°ï¼‰
        - hidden_sizesï¼šéšè—å±‚å¤§å°åˆ—è¡¨ï¼Œä¾‹å¦‚[64, 32]è¡¨ç¤ºä¸¤ä¸ªéšè—å±‚ï¼Œå¤§å°åˆ†åˆ«ä¸º64å’Œ32
        - output_sizeï¼šè¾“å‡ºå±‚å¤§å°ï¼ˆç±»åˆ«æ•°ï¼‰
        - activation_functionï¼šæ¿€æ´»å‡½æ•°ï¼Œé»˜è®¤ä¸ºReLU
        """
        super(MLP, self).__init__()
        self.layers = nn.ModuleList()
        in_size = input_size

        # æž„å»ºéšè—å±‚
        for hidden_size in hidden_sizes:
            self.layers.append(nn.Linear(in_size, hidden_size))
            in_size = hidden_size
        # æž„å»ºè¾“å‡ºå±‚
        self.out_layer = nn.Linear(in_size, output_size)
        # æ¿€æ´»å‡½æ•°
        self.activation = activation_function

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­å‡½æ•°
        å‚æ•°ï¼š
        - xï¼šè¾“å…¥å¼ é‡
        è¿”å›žï¼š
        - è¾“å‡ºå¼ é‡
        """
        for layer in self.layers:
            x = self.activation(layer(x))
        x = self.out_layer(x)
        return x


# nn.Linearæ¨¡å—ç”¨äºŽå®žçŽ°ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œå³å…¨è¿žæŽ¥å±‚
# å®šä¹‰ä¸ºï¼šnn.Linear(in_features, out_features, bias=True)
# å‚æ•°ï¼š
# - in_featuresï¼šè¾“å…¥çš„ç‰¹å¾æ•°
# - out_featuresï¼šè¾“å‡ºçš„ç‰¹å¾æ•°
# - biasï¼šæ˜¯å¦åŒ…å«åç½®é¡¹ï¼Œé»˜è®¤ä¸ºTrue

# æ¿€æ´»å‡½æ•°å¯ä»¥æ˜¯nnæ¨¡å—ä¸­çš„å„ç§æ¿€æ´»å‡½æ•°ï¼Œä¾‹å¦‚ï¼š
# - nn.ReLU()
# - nn.Sigmoid()
# - nn.Tanh()
# - nn.PReLU()

# åœ¨forwardå‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å°†è¾“å…¥ä¾æ¬¡é€šè¿‡éšè—å±‚å’Œæ¿€æ´»å‡½æ•°ï¼Œç„¶åŽé€šè¿‡è¾“å‡ºå±‚

# åˆ›å»ºä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºå®žä¾‹
input_size = 784  # è¾“å…¥å±‚å¤§å°ï¼Œä¾‹å¦‚MNISTæ•°æ®é›†çš„28x28åƒç´ å±•å¼€ä¸º784ç»´
hidden_sizes = [256, 128]  # ä¸¤ä¸ªéšè—å±‚ï¼Œå¤§å°åˆ†åˆ«ä¸º256å’Œ128
output_size = 10  # è¾“å‡ºå±‚å¤§å°ï¼Œä¾‹å¦‚10åˆ†ç±»

# ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
mlp_model = MLP(input_size, hidden_sizes, output_size, activation_function=nn.ReLU())

print("å¤šå±‚æ„ŸçŸ¥æœºæ¨¡åž‹ç»“æž„ï¼š")
print(mlp_model)

# ç”Ÿæˆéšæœºè¾“å…¥æ•°æ®ï¼Œæ‰¹é‡å¤§å°ä¸º64
batch_size = 64
x = torch.randn(batch_size, input_size)

# å‰å‘ä¼ æ’­
output = mlp_model(x)

print("æ¨¡åž‹è¾“å‡ºçš„å½¢çŠ¶ï¼š", output.shape)

# ä½¿ç”¨Sigmoidæ¿€æ´»å‡½æ•°
mlp_model_sigmoid = MLP(input_size, hidden_sizes, output_size, activation_function=nn.Sigmoid())

print("ä½¿ç”¨Sigmoidæ¿€æ´»å‡½æ•°çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡åž‹ç»“æž„ï¼š")
print(mlp_model_sigmoid)

# å‰å‘ä¼ æ’­
output_sigmoid = mlp_model_sigmoid(x)

print("ä½¿ç”¨Sigmoidæ¿€æ´»å‡½æ•°çš„æ¨¡åž‹è¾“å‡ºå½¢çŠ¶ï¼š", output_sigmoid.shape)

# -------------------------------
print("ç»ƒä¹ 1ï¼šè®¡ç®—PReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°")
# -------------------------------

# PReLUæ¿€æ´»å‡½æ•°å®šä¹‰ä¸ºï¼š
# PReLU(x) = max(0, x) + Î± * min(0, x)

# å®ƒçš„å¯¼æ•°ä¸ºï¼š
# å½“x > 0æ—¶ï¼Œå¯¼æ•°ä¸º1
# å½“x < 0æ—¶ï¼Œå¯¼æ•°ä¸ºÎ±
# å½“x = 0æ—¶ï¼Œå¯¼æ•°æœªå®šä¹‰ï¼Œé€šå¸¸å–1æˆ–Î±

# æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»˜åˆ¶å¯¼æ•°æ¥éªŒè¯

# èŽ·å–Î±çš„å€¼
alpha = prelu.weight.item()

# è®¡ç®—å¯¼æ•°
x_np = x.detach().numpy()
grad_prelu = np.ones_like(x_np)
grad_prelu[x_np < 0] = alpha

# ç»˜åˆ¶PReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°
plt.figure(figsize=(5, 2.5))
plt.plot(x_np.flatten(), grad_prelu.flatten())
plt.title('PReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•° (æ‰‹åŠ¨è®¡ç®—)')
plt.xlabel('x')
plt.ylabel('grad of PReLU(x)')
plt.show()

# -------------------------------
print("ç»ƒä¹ 2ï¼šè¯æ˜Žä½¿ç”¨ReLUçš„å¤šå±‚æ„ŸçŸ¥æœºæž„é€ äº†ä¸€ä¸ªè¿žç»­çš„åˆ†æ®µçº¿æ€§å‡½æ•°")


# -------------------------------

# å®šä¹‰ä¸€ä¸ªç®€å•çš„MLPï¼Œè¾“å…¥ä¸º1ç»´ï¼Œè¾“å‡ºä¸º1ç»´
class SimpleMLP(nn.Module):
    def __init__(self):
        super(SimpleMLP, self).__init__()
        self.layer1 = nn.Linear(1, 10)
        self.layer2 = nn.Linear(10, 1)
        self.activation = nn.ReLU()

    def forward(self, x):
        x = self.activation(self.layer1(x))
        x = self.layer2(x)
        return x


# å®žä¾‹åŒ–æ¨¡åž‹
simple_mlp = SimpleMLP()

# ç”Ÿæˆè¾“å…¥èŒƒå›´
x_input = torch.unsqueeze(torch.linspace(-5, 5, 200), dim=1)

# å‰å‘ä¼ æ’­
y_output = simple_mlp(x_input)

# ç»˜åˆ¶è¾“å‡ºå‡½æ•°
plt.figure(figsize=(5, 2.5))
plt.plot(x_input.detach().numpy(), y_output.detach().numpy())
plt.title('ä½¿ç”¨ReLUçš„MLPè¾“å‡º')
plt.xlabel('x')
plt.ylabel('y')
plt.show()

# ç”±äºŽReLUå’Œçº¿æ€§å±‚çš„ç»„åˆï¼Œè¾“å‡ºå‡½æ•°æ˜¯è¿žç»­çš„åˆ†æ®µçº¿æ€§å‡½æ•°

# -------------------------------
print("ç»ƒä¹ 3ï¼šè¯æ˜Ž tanh(x) + 1 = 2 * sigmoid(2x)")
# -------------------------------

x_values = torch.linspace(-5, 5, 100)

lhs = torch.tanh(x_values) + 1
rhs = 2 * torch.sigmoid(2 * x_values)

# ç»˜åˆ¶æ¯”è¾ƒå›¾
plt.figure(figsize=(5, 2.5))
plt.plot(x_values.detach().numpy(), lhs.detach().numpy(), label='tanh(x) + 1')
plt.plot(x_values.detach().numpy(), rhs.detach().numpy(), label='2 * sigmoid(2x)', linestyle='--')
plt.title('tanh(x) + 1 ä¸Ž 2 * sigmoid(2x) æ¯”è¾ƒ')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()

# éªŒè¯ä¸¤è€…çš„å·®å¼‚
difference = torch.abs(lhs - rhs)
print("æœ€å¤§å·®å¼‚ï¼š", torch.max(difference).item())

# -------------------------------
print("ç»ƒä¹ 4ï¼šéžçº¿æ€§å•å…ƒä¸€æ¬¡åº”ç”¨äºŽä¸€ä¸ªå°æ‰¹é‡æ•°æ®å¯èƒ½å¯¼è‡´çš„é—®é¢˜")


# -------------------------------

# å®šä¹‰ä¸€ä¸ªéžçº¿æ€§å‡½æ•°ï¼Œé”™è¯¯åœ°ä½œç”¨äºŽæ•´ä¸ªæ‰¹é‡æ•°æ®
def nonlinear_batch_function(x):
    # ä¾‹å¦‚ï¼Œå¯¹æ•´ä¸ªæ‰¹é‡è®¡ç®—softmax
    return torch.softmax(x, dim=0)


# ç”Ÿæˆä¸€ä¸ªå°æ‰¹é‡æ•°æ®ï¼Œå½¢çŠ¶ä¸º(batch_size, features)
batch_size = 4
features = 2
x_batch = torch.randn(batch_size, features)

# åº”ç”¨éžçº¿æ€§å‡½æ•°
y_batch = nonlinear_batch_function(x_batch)

print("è¾“å…¥x_batchï¼š")
print(x_batch)
print("è¾“å‡ºy_batchï¼š")
print(y_batch)

# é—®é¢˜åœ¨äºŽï¼Œéžçº¿æ€§å‡½æ•°ä½œç”¨åœ¨æ•´ä¸ªæ‰¹é‡ä¸Šï¼Œä¼šå¯¼è‡´æ ·æœ¬é—´çš„ä¿¡æ¯æ··åˆï¼Œç ´åäº†æ ·æœ¬çš„ç‹¬ç«‹æ€§

# æ€»ç»“ï¼š
# æœ¬ä»£ç ç¤ºä¾‹ä¸­ä½¿ç”¨äº†ä»¥ä¸‹å‡½æ•°å’Œæ¨¡å—ï¼š

# 1. torch.nn.functional.relu(input, inplace=False)
#    - è®¡ç®—ReLUæ¿€æ´»å‡½æ•°
#    - å‚æ•°ï¼š
#      - inputï¼šè¾“å…¥å¼ é‡
#      - inplaceï¼šæ˜¯å¦è¿›è¡ŒåŽŸåœ°æ“ä½œï¼Œé»˜è®¤ä¸ºFalse
#    - ç¤ºä¾‹ï¼š
#      y = F.relu(x)

# 2. torch.sigmoid(input)
#    - è®¡ç®—Sigmoidæ¿€æ´»å‡½æ•°
#    - å‚æ•°ï¼š
#      - inputï¼šè¾“å…¥å¼ é‡
#    - ç¤ºä¾‹ï¼š
#      y = torch.sigmoid(x)

# 3. torch.tanh(input)
#    - è®¡ç®—Tanhæ¿€æ´»å‡½æ•°
#    - å‚æ•°ï¼š
#      - inputï¼šè¾“å…¥å¼ é‡
#    - ç¤ºä¾‹ï¼š
#      y = torch.tanh(x)

# 4. torch.nn.PReLU(num_parameters=1, init=0.25)
#    - å®šä¹‰PReLUæ¿€æ´»å‡½æ•°
#    - å‚æ•°ï¼š
#      - num_parametersï¼šÎ±å‚æ•°çš„æ•°é‡ï¼Œå¯ä»¥æ˜¯1æˆ–è¾“å…¥é€šé“æ•°
#      - initï¼šÎ±çš„åˆå§‹å€¼ï¼Œé»˜è®¤ä¸º0.25
#    - ç¤ºä¾‹ï¼š
#      prelu = nn.PReLU(num_parameters=1, init=0.25)
#      y = prelu(x)

# 5. nn.Linear(in_features, out_features, bias=True)
#    - å®šä¹‰å…¨è¿žæŽ¥å±‚ï¼ˆçº¿æ€§å˜æ¢ï¼‰
#    - å‚æ•°ï¼š
#      - in_featuresï¼šè¾“å…¥çš„ç‰¹å¾æ•°
#      - out_featuresï¼šè¾“å‡ºçš„ç‰¹å¾æ•°
#      - biasï¼šæ˜¯å¦åŒ…å«åç½®é¡¹ï¼Œé»˜è®¤ä¸ºTrue
#    - ç¤ºä¾‹ï¼š
#      linear = nn.Linear(10, 5)
#      y = linear(x)

# 6. torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False)
#    - è®¡ç®—æ¢¯åº¦
#    - å‚æ•°ï¼š
#      - tensorsï¼šéœ€è¦è®¡ç®—æ¢¯åº¦çš„å¼ é‡
#      - grad_tensorsï¼šå…³äºŽæ¯ä¸ªå…ƒç´ çš„æ¢¯åº¦æƒé‡ï¼Œé»˜è®¤ä¸º1
#    - ç¤ºä¾‹ï¼š
#      y.backward(torch.ones_like(x))

# 7. torch.softmax(input, dim=None)
#    - è®¡ç®—Softmaxå‡½æ•°
#    - å‚æ•°ï¼š
#      - inputï¼šè¾“å…¥å¼ é‡
#      - dimï¼šè®¡ç®—Softmaxçš„ç»´åº¦
#    - ç¤ºä¾‹ï¼š
#      y = torch.softmax(x, dim=1)

# æœ¬ä»£ç ç¤ºä¾‹å±•ç¤ºäº†å¤šç§æ¿€æ´»å‡½æ•°çš„ä½¿ç”¨æ–¹æ³•ã€å…¶å¯¼æ•°çš„è®¡ç®—ä»¥åŠåœ¨å¤šå±‚æ„ŸçŸ¥æœºä¸­çš„åº”ç”¨ã€‚
# é€šè¿‡ç»ƒä¹ ï¼Œæ·±å…¥ç†è§£äº†PReLUæ¿€æ´»å‡½æ•°çš„å¯¼æ•°è®¡ç®—ï¼Œè¯æ˜Žäº†ä½¿ç”¨ReLUçš„å¤šå±‚æ„ŸçŸ¥æœºæž„é€ äº†ä¸€ä¸ªè¿žç»­çš„åˆ†æ®µçº¿æ€§å‡½æ•°ï¼Œ
# éªŒè¯äº†tanhå’Œsigmoidå‡½æ•°ä¹‹é—´çš„å…³ç³»ï¼Œå¹¶è¯´æ˜Žäº†éžçº¿æ€§å‡½æ•°é”™è¯¯åœ°ä½œç”¨äºŽæ•´ä¸ªæ‰¹é‡æ•°æ®å¯èƒ½å¯¼è‡´çš„é—®é¢˜ã€‚
